from difflib import SequenceMatcher
import yaml
from pathlib import Path
from PIL import Image
from io import BytesIO
import requests
import hashlib
import re
import logging
import csv
from collections import defaultdict

class ProductMatcher:
    def __init__(self, config_path=None):
        # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–loggerï¼ˆå¿…é¡»åœ¨å…¶ä»–æ“ä½œä¹‹å‰ï¼‰
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            ch = logging.StreamHandler()
            ch.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            self.logger.addHandler(ch)
        
        # ç¬¬äºŒæ­¥ï¼šè®¾ç½®é…ç½®è·¯å¾„
        if not config_path:
            config_path = Path(__file__).parent.parent / "config" / "rules.yaml"
        
        # ç¬¬ä¸‰æ­¥ï¼šåŠ è½½é…ç½®æ–‡ä»¶
        self.rules = self._load_config(config_path)
        
        # ç¬¬å››æ­¥ï¼šè®¾ç½®é»˜è®¤å€¼
        self._set_defaults()
        
        # ç¬¬äº”æ­¥ï¼šåˆå§‹åŒ–å›¾ç‰‡å“ˆå¸Œç¼“å­˜
        self.image_hash_cache = {}

    def _load_config(self, config_path):
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        encodings = ['utf-8', 'utf-8-sig', 'gb18030', 'latin-1']
        for encoding in encodings:
            try:
                with open(config_path, 'r', encoding=encoding) as f:
                    rules = yaml.safe_load(f) or {}
                    self.logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½é…ç½®")
                    return rules
            except UnicodeDecodeError:
                continue
            except yaml.YAMLError as e:
                self.logger.error(f"YAMLè§£æé”™è¯¯: {e}")
                raise
            except FileNotFoundError:
                self.logger.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
                raise
        
        raise ValueError(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}ï¼Œå°è¯•äº†å¤šç§ç¼–ç å‡å¤±è´¥")

    def _set_defaults(self):
        """è®¾ç½®é»˜è®¤é…ç½®å€¼"""
        self.rules.setdefault('matching', {})
        self.rules['matching'].setdefault('similarity_threshold', 0.5)
        
        # è®¾ç½®å„åŒ¹é…ç»´åº¦çš„æƒé‡
        weights = self.rules['matching'].setdefault('weights', {})
        weights.setdefault('title', 0.4)
        weights.setdefault('description', 0.2)
        weights.setdefault('price', 0.2)
        weights.setdefault('images', 0.2)
        
        # è®¾ç½®ä»·æ ¼åŒ¹é…å‚æ•°
        price_rules = self.rules['matching'].setdefault('price_rules', {})
        price_rules.setdefault('max_percentage_diff', 0.9)  # ä»·æ ¼æœ€å¤§å·®å¼‚ç™¾åˆ†æ¯”
        price_rules.setdefault('allow_free_shipping_diff', True)
        
        # è®¾ç½®å›¾ç‰‡åŒ¹é…å‚æ•°
        image_rules = self.rules['matching'].setdefault('image_rules', {})
        image_rules.setdefault('hash_size', 32)  # å“ˆå¸Œå¤§å° (16x16 = 256ä½)
        image_rules.setdefault('min_similar_images', 1)  # æœ€å°ç›¸ä¼¼å›¾ç‰‡æ•°
        image_rules.setdefault('similarity_threshold', 0.8)  # å›¾ç‰‡ç›¸ä¼¼åº¦é˜ˆå€¼

    def is_same_product(self, product1, product2):
        """åˆ¤æ–­ä¸¤ä¸ªå•†å“æ˜¯å¦ç›¸ä¼¼"""
        # æå–åŒ¹é…æ‰€éœ€çš„å•†å“å±æ€§
        p1 = self._extract_product_features(product1)
        p2 = self._extract_product_features(product2)
        # è®¡ç®—å„ç»´åº¦ç›¸ä¼¼åº¦
        title_sim = self._compare_text(p1['title'], p2['title'])
        desc_sim = self._compare_text(p1['description'], p2['description'])
        price_sim = self._compare_prices(p1['price'], p2['price'])
        image_sim = self._compare_images(p1['images'], p2['images'])
        
        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        weights = self.rules['matching']['weights']
        overall_similarity = (
            title_sim * weights['title'] +
            desc_sim * weights['description'] +
            price_sim * weights['price'] +
            image_sim * weights['images']
        )* (0.5 if p1['platform'] == p2['platform'] else 1.2) 
        
        # è®°å½•åŒ¹é…è¯¦æƒ…
        self.logger.debug(f"å•†å“åŒ¹é…è¯¦æƒ…: "
                         f"æ ‡é¢˜ç›¸ä¼¼åº¦={title_sim:.2f}, "
                         f"æè¿°ç›¸ä¼¼åº¦={desc_sim:.2f}, "
                         f"ä»·æ ¼ç›¸ä¼¼åº¦={price_sim:.2f}, "
                         f"å›¾ç‰‡ç›¸ä¼¼åº¦={image_sim:.2f}, "
                         f"ç»¼åˆç›¸ä¼¼åº¦={overall_similarity:.2f}")
        
        return {
            'is_same': overall_similarity >= self.rules['matching']['similarity_threshold'],
            'similarity': overall_similarity,
            'breakdown': {
                'title': title_sim,
                'description': desc_sim,
                'price': price_sim,
                'images': image_sim
            }
        }

    def _extract_product_features(self, product):
        """æå–å’Œæ ‡å‡†åŒ–å•†å“ç‰¹å¾"""
        title = re.sub(r'(202[0-9]|æ–°æ¬¾|æ­£å“|æ˜¥å­£|å¤å­£|ç§‹å­£|å†¬å­£|æ½®æµ|\d+|[a-zA-Z]+)', '', 
                  product.get('title', '').strip().lower())
        return {
            'platform': product.get('platform'),
            'title': product.get('title', '').strip().lower(),
            'description': product.get('description', '').strip().lower(),
            'price': self._parse_price(product.get('price', '')),
            'images': product.get('images', [])
        }

    def _parse_price(self, price_text):
        """è§£æä»·æ ¼æ–‡æœ¬ä¸ºæµ®ç‚¹æ•°"""
        if not price_text:
            return None
        
        # å¤„ç†å¸¸è§ä»·æ ¼æ ¼å¼ï¼ˆå¦‚ "Â¥1,234.56", "$1234.56"ï¼‰
        try:
            # ç§»é™¤è´§å¸ç¬¦å·å’Œåƒä½åˆ†éš”ç¬¦
            price_text = re.sub(r'[^\d\.]', '', price_text)
            return float(price_text)
        except (ValueError, TypeError):
            #self.logger.warning(f"æ— æ³•è§£æä»·æ ¼: {price_text}")
            return None

    def _compare_text(self, text1, text2):
        """æ¯”è¾ƒä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0
        
        # åŸºç¡€ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, text1, text2).ratio()
        
        # å¢å¼ºåŒ¹é…ï¼šæ£€æŸ¥å…³é”®å±æ€§æ˜¯å¦å®Œå…¨åŒ¹é…
        if similarity > 0.5:  # åªå¯¹ä¸­ç­‰ç›¸ä¼¼åº¦ä»¥ä¸Šçš„æ–‡æœ¬è¿›è¡Œå¢å¼ºæ£€æŸ¥
            # æå–å…³é”®å±æ€§ï¼ˆå¦‚å“ç‰Œã€å‹å·ç­‰ï¼‰
            # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚å®šåˆ¶
            keywords = ['brand', 'model', 'color', 'size']
            keyword_matches = 0
            keyword_count = 0
            
            for keyword in keywords:
                pattern = re.compile(rf'\b{keyword}[:=]\s*([^\s,]+)\b', re.IGNORECASE)
                match1 = pattern.search(text1)
                match2 = pattern.search(text2)
                
                if match1 and match2:
                    keyword_count += 1
                    if match1.group(1).lower() == match2.group(1).lower():
                        keyword_matches += 1
            
            # å¦‚æœæœ‰å…³é”®å±æ€§åŒ¹é…ï¼Œæé«˜ç›¸ä¼¼åº¦
            if keyword_count > 0:
                keyword_bonus = keyword_matches / keyword_count * 0.2
                similarity = min(1.0, similarity + keyword_bonus)
        
        return similarity

    def _compare_prices(self, price1, price2):
        """æ¯”è¾ƒä¸¤ä¸ªä»·æ ¼çš„ç›¸ä¼¼åº¦"""
        if price1 is None or price2 is None:
            return 0.5  # æ— æ³•æ¯”è¾ƒæ—¶è¿”å›ä¸­ç­‰ç›¸ä¼¼åº¦
        
        if price1 == 0 and price2 == 0:
            return 1.0
        
        max_diff = self.rules['matching']['price_rules']['max_percentage_diff']
        
        # è®¡ç®—ä»·æ ¼å·®å¼‚ç™¾åˆ†æ¯”
        price_diff = abs(price1 - price2) / max(price1, price2)
        
        # å¦‚æœå·®å¼‚åœ¨å…è®¸èŒƒå›´å†…ï¼Œè¿”å›ç›¸ä¼¼åº¦
        if price_diff <= max_diff:
            return 1.0 - price_diff
        
        return 0.0

    def _compare_images(self, images1, images2):
        """æ¯”è¾ƒä¸¤ç»„å›¾ç‰‡çš„ç›¸ä¼¼åº¦"""
        if not images1 or not images2:
            return 0
        
        hash_size = self.rules['matching']['image_rules']['hash_size']
        min_similar = self.rules['matching']['image_rules']['min_similar_images']
        image_threshold = self.rules['matching']['image_rules']['similarity_threshold']
        
        # è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„å“ˆå¸Œå€¼
        hashes1 = [self._get_image_hash(url, hash_size) for url in images1]
        hashes2 = [self._get_image_hash(url, hash_size) for url in images2]
        
        # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å›¾ç‰‡å¯¹
        max_similarities = []
        for h1 in hashes1:
            if not h1:
                continue
            similarities = []
            for h2 in hashes2:
                if not h2:
                    continue
                similarity = self._compare_image_hashes(h1, h2)
                similarities.append(similarity)
            
            if similarities:
                max_similarities.append(max(similarities))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„å›¾ç‰‡å¯¹
        if not max_similarities:
            return 0
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ï¼ˆåªè€ƒè™‘è¶…è¿‡é˜ˆå€¼çš„å›¾ç‰‡å¯¹ï¼‰
        valid_similarities = [s for s in max_similarities if s >= image_threshold]
        
        if not valid_similarities:
            return 0
        
        # å¦‚æœç›¸ä¼¼å›¾ç‰‡æ•°é‡ä¸è¶³ï¼Œè¿”å›0
        if len(valid_similarities) < min_similar:
            return 0
        
        return sum(valid_similarities) / len(valid_similarities)

    def _get_image_hash(self, url, hash_size=16):
        """è®¡ç®—å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œå€¼"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{url}_{hash_size}"
        if cache_key in self.image_hash_cache:
            return self.image_hash_cache[cache_key]
        
        try:
            # å°è¯•è·å–å›¾ç‰‡å†…å®¹
            response = requests.get(url, timeout=5)
            response.raise_for_status()
            
            # è®¡ç®—å“ˆå¸Œ
            img = Image.open(BytesIO(response.content))
            img = img.resize((hash_size, hash_size), Image.LANCZOS).convert('L')
            pixels = list(img.getdata())
            avg = sum(pixels) / len(pixels)
            bits = "".join(['1' if (px >= avg) else '0' for px in pixels])
            
            # ç¼“å­˜ç»“æœ
            self.image_hash_cache[cache_key] = bits
            return bits
        except Exception as e:
            self.logger.warning(f"æ— æ³•è·å–æˆ–å¤„ç†å›¾ç‰‡ {url}: {e}")
            return None

    def _compare_image_hashes(self, hash1, hash2):
        """æ¯”è¾ƒä¸¤ä¸ªå›¾ç‰‡å“ˆå¸Œçš„ç›¸ä¼¼åº¦"""
        if not hash1 or not hash2:
            return 0
        
        # è®¡ç®—æ±‰æ˜è·ç¦»
        distance = sum(c1 != c2 for c1, c2 in zip(hash1, hash2))
        similarity = 1.0 - (distance / len(hash1))
        
        return similarity

def standardize_product_data(product):
    """æ ‡å‡†åŒ–å•†å“æ•°æ®æ ¼å¼"""
    return {
        'goods_title': str(product.get('goods_title', '')).strip(),
        'goods_price': float(product.get('goods_price', 0)),
        'goods_sales': str(product.get('goods_sales', '0')),
        'goods_img': str(product.get('goods_img', '')),
        'shop_title': str(product.get('shop_title', '')).strip(),
        'shop_platform': str(product.get('shop_platform', '')).strip(),
        'goods_link': str(product.get('goods_link', '')),
        'grab_time': str(product.get('grab_time', '')),
        'page_type': str(product.get('page_type', 'product')),
        'search_keyword': str(product.get('search_keyword', '')).strip(),
        'brand': str(product.get('brand', 'æœªçŸ¥')).strip()
    }

def group_products_by_similarity(suning_list, pdd_list, matcher=None):
    """
    å…ˆæŒ‰å“ç‰Œç²—åˆ†ç»„ï¼Œå†åœ¨å“ç‰Œç»„å†…ç”¨ProductMatcherç»†åˆ†ï¼Œå…è®¸è·¨å¹³å°å•†å“åˆ†åˆ°åŒç»„ã€‚
    è¿”å›åˆ†ç»„å­—å…¸ï¼š{åˆ†ç»„key: [å•†å“åˆ—è¡¨]}
    """
    if matcher is None:
        matcher = ProductMatcher()
    
    # æ ‡å‡†åŒ–å•†å“æ•°æ®æ ¼å¼
    def standardize_product(product, platform):
        standardized = standardize_product_data(product)
        return {
            'title': standardized['goods_title'],
            'description': standardized['goods_title'],  # ç”¨æ ‡é¢˜ä½œä¸ºæè¿°
            'price': standardized['goods_price'],
            'images': [standardized['goods_img']] if standardized['goods_img'] else [],
            'platform': platform,
            'brand': standardized['brand'],
            'original_data': standardized
        }
    
    # æ ‡å‡†åŒ–æ‰€æœ‰å•†å“
    all_products = []
    for product in suning_list:
        all_products.append(standardize_product(product, 'è‹å®'))
    for product in pdd_list:
        all_products.append(standardize_product(product, 'æ‹¼å¤šå¤š'))
    
    # 1. å…ˆæŒ‰å“ç‰Œç²—åˆ†ç»„
    brand_groups = defaultdict(list)
    for idx, product in enumerate(all_products):
        brand = product['brand'] if product['brand'] else 'æœªçŸ¥'
        brand_groups[brand].append((idx, product))
    
    # 2. æ¯ä¸ªå“ç‰Œç»„å†…å†ç”¨ProductMatcherç»†åˆ†
    groups = defaultdict(list)
    used_products = set()
    group_id = 1
    for brand, prod_list in brand_groups.items():
        n = len(prod_list)
        for i in range(n):
            idx1, product1 = prod_list[i]
            if idx1 in used_products:
                continue
            group_key = f"group_{group_id}"
            groups[group_key].append(product1)
            used_products.add(idx1)
            for j in range(i+1, n):
                idx2, product2 = prod_list[j]
                if idx2 in used_products:
                    continue
                match_result = matcher.is_same_product(product1, product2)
                if match_result['is_same']:
                    groups[group_key].append(product2)
                    used_products.add(idx2)
            group_id += 1
    
    # 3. æŠŠæœªåˆ†ç»„çš„å•†å“å•ç‹¬æˆç»„ï¼ˆå¦‚å“ç‰ŒæœªçŸ¥ï¼‰
    for idx, product in enumerate(all_products):
        if idx not in used_products:
            group_key = f"group_{group_id}"
            groups[group_key].append(product)
            group_id += 1
    return groups

def print_group_debug_info(groups):
    """æ‰“å°åˆ†ç»„è°ƒè¯•ä¿¡æ¯"""
    print("\n===== å•†å“åˆ†ç»„å¯¹æ¯”è°ƒè¯•ä¿¡æ¯ =====")
    for idx, (key, group) in enumerate(groups.items(), 1):
        # æ£€æŸ¥æ˜¯å¦ä¸ºè·¨å¹³å°å•†å“ç»„
        platforms = set(g['platform'] for g in group)
        is_cross_platform = len(platforms) > 1
        platform_label = "ã€è·¨å¹³å°ã€‘" if is_cross_platform else "ã€å•å¹³å°ã€‘"
        
        print(f"\nåˆ†ç»„{idx}: {key} {platform_label}")
        print(f"  å¹³å°åˆ†å¸ƒ: {', '.join(platforms)}")
        
        # ç»Ÿè®¡å“ç‰Œåˆ†å¸ƒ
        brands = set(g['original_data'].get('brand', 'æœªçŸ¥') for g in group)
        print(f"  å“ç‰Œåˆ†å¸ƒ: {', '.join(brands)}")
        
        # ä»·æ ¼ç»Ÿè®¡
        prices = [float(g['original_data'].get('goods_price', 0)) for g in group]
        max_price = max(prices)
        min_price = min(prices)
        max_item = group[prices.index(max_price)]
        min_item = group[prices.index(min_price)]
        
        print(f"  å•†å“æ•°é‡: {len(group)}")
        print(f"  ä»·æ ¼èŒƒå›´: {min_price:.2f} - {max_price:.2f}")
        print(f"  æœ€é«˜ä»·: {max_item['original_data'].get('goods_price')} | æ ‡é¢˜: {max_item['original_data'].get('goods_title')} | å¹³å°: {max_item['original_data'].get('shop_platform')}")
        print(f"  æœ€ä½ä»·: {min_item['original_data'].get('goods_price')} | æ ‡é¢˜: {min_item['original_data'].get('goods_title')} | å¹³å°: {min_item['original_data'].get('shop_platform')}")
        print("  æ‰€æœ‰å•†å“:")
        for g in group:
            original = g['original_data']
            print(f"    - {original.get('shop_platform')} | {original.get('brand')} | {original.get('goods_title')} | {original.get('goods_price')}")
    
    print("\n===== ç»“æŸ =====\n")

def save_goods_to_csv(goods_list, filename='compared_goods.csv'):
    """ä¿å­˜æœ€ä½ä»·å•†å“åˆ°CSV"""
    if not goods_list:
        return False
    
    fieldnames = [
        'goods_img', 'goods_title', 'goods_price', 'goods_sales',
        'shop_title', 'shop_platform', 'goods_link', 'grab_time',
        'page_type', 'search_keyword', 'brand'
    ]
    
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        for row in goods_list:
            writer.writerow({k: row.get(k, '') for k in fieldnames})
    
    return True

def compare_and_save(suning_list, pdd_list, csv_filename='compared_goods.csv'):
    """
    ç»¼åˆæµç¨‹ï¼šä½¿ç”¨ProductMatcheråˆ†ç»„ã€ç­›é€‰æœ€ä½ä»·ã€ä¿å­˜åˆ°CSVï¼Œè¿”å›æœ€ä½ä»·å•†å“åˆ—è¡¨å’Œè°ƒè¯•ä¿¡æ¯
    ä¼˜å…ˆæ˜¾ç¤ºè·¨å¹³å°å•†å“
    """
    # æ•°æ®æ ‡å‡†åŒ–æ¸…æ´—
    suning_list = [standardize_product_data(item) for item in suning_list]
    pdd_list = [standardize_product_data(item) for item in pdd_list]
    
    # ä½¿ç”¨ProductMatcherè¿›è¡Œåˆ†ç»„
    groups = group_products_by_similarity(suning_list, pdd_list)
    
    # ç­›é€‰æ¯ç»„æœ€ä½ä»·å•†å“ï¼Œå¹¶ä¼˜å…ˆè·¨å¹³å°å•†å“
    min_goods = []
    debug_info = []
    
    # åˆ†ç¦»è·¨å¹³å°å•†å“ç»„å’Œå•å¹³å°å•†å“ç»„
    cross_platform_groups = []
    single_platform_groups = []
    
    for key, group in groups.items():
        # æ£€æŸ¥æ˜¯å¦ä¸ºè·¨å¹³å°å•†å“ç»„
        platforms = set(g['platform'] for g in group)
        is_cross_platform = len(platforms) > 1
        
        prices = [float(g['original_data'].get('goods_price', 0)) for g in group]
        min_item = group[prices.index(min(prices))]
        
        # æ”¶é›†è°ƒè¯•ä¿¡æ¯
        brands = set(g['original_data'].get('brand', 'æœªçŸ¥') for g in group)
        group_info = {
            'group_key': key,
            'brands': list(brands),
            'max_price': max(prices),
            'min_price': min(prices),
            'max_item': group[prices.index(max(prices))]['original_data'],
            'min_item': min_item['original_data'],
            'count': len(group),
            'all_items': [g['original_data'] for g in group],
            'is_cross_platform': is_cross_platform,
            'platforms': list(platforms)
        }
        
        if is_cross_platform:
            cross_platform_groups.append((key, group, min_item, group_info))
        else:
            single_platform_groups.append((key, group, min_item, group_info))
    
    # ä¼˜å…ˆæ·»åŠ è·¨å¹³å°å•†å“ï¼Œç„¶åæ·»åŠ å•å¹³å°å•†å“
    for key, group, min_item, group_info in cross_platform_groups + single_platform_groups:
        min_goods.append(min_item['original_data'])
        debug_info.append(group_info)
    
    # ä¿å­˜åˆ°CSV
    save_goods_to_csv(min_goods, filename=csv_filename)
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print_group_debug_info(groups)
    
    # æ‰“å°è·¨å¹³å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n===== è·¨å¹³å°å•†å“ç»Ÿè®¡ =====")
    print(f"è·¨å¹³å°å•†å“ç»„æ•°é‡: {len(cross_platform_groups)}")
    print(f"å•å¹³å°å•†å“ç»„æ•°é‡: {len(single_platform_groups)}")
    print(f"æ€»å•†å“ç»„æ•°é‡: {len(groups)}")
    print(f"è·¨å¹³å°å•†å“å æ¯”: {len(cross_platform_groups)/len(groups)*100:.1f}%")
    print("===== ç»“æŸ =====\n")
    
    return min_goods, debug_info

def simple_mix_products(suning_list, pdd_list, mix_ratio=None):
    """
    ç®€å•æ··åˆå•†å“æ•°æ®
    :param suning_list: è‹å®å•†å“åˆ—è¡¨
    :param pdd_list: æ‹¼å¤šå¤šå•†å“åˆ—è¡¨
    :param mix_ratio: æ··åˆæ¯”ä¾‹ï¼Œå¦‚ {'è‹å®': 0.4, 'æ‹¼å¤šå¤š': 0.6}ï¼Œé»˜è®¤éšæœºæ··åˆ
    :return: æ··åˆåçš„å•†å“åˆ—è¡¨
    """
    print("ğŸ² å¼€å§‹ç®€å•æ··åˆå•†å“æ•°æ®...")
    
    # æ•°æ®æ ‡å‡†åŒ–
    suning_list = [standardize_product_data(item) for item in suning_list]
    pdd_list = [standardize_product_data(item) for item in pdd_list]
    
    print(f"åŸå§‹æ•°æ®: è‹å® {len(suning_list)} ä¸ª, æ‹¼å¤šå¤š {len(pdd_list)} ä¸ª")
    
    # åˆå¹¶æ‰€æœ‰å•†å“
    all_products = []
    
    # æ·»åŠ è‹å®å•†å“
    for product in suning_list:
        all_products.append(product)
    
    # æ·»åŠ æ‹¼å¤šå¤šå•†å“
    for product in pdd_list:
        all_products.append(product)
    
    # éšæœºæ‰“ä¹±é¡ºåº
    import random
    random.shuffle(all_products)
    
    print(f"æ··åˆå®Œæˆ: æ€»å…± {len(all_products)} ä¸ªå•†å“")
    
    # ç»Ÿè®¡å¹³å°åˆ†å¸ƒ
    suning_count = len([p for p in all_products if p['shop_platform'] == 'è‹å®'])
    pdd_count = len([p for p in all_products if p['shop_platform'] == 'æ‹¼å¤šå¤š'])
    
    print(f"æ··åˆååˆ†å¸ƒ: è‹å® {suning_count} ä¸ª, æ‹¼å¤šå¤š {pdd_count} ä¸ª")
    
    return all_products

def smart_mix_products(suning_list, pdd_list, target_ratio=None):
    """
    æ™ºèƒ½æ··åˆå•†å“æ•°æ®ï¼ˆä¿æŒæŒ‡å®šæ¯”ä¾‹ï¼‰
    :param suning_list: è‹å®å•†å“åˆ—è¡¨
    :param pdd_list: æ‹¼å¤šå¤šå•†å“åˆ—è¡¨
    :param target_ratio: ç›®æ ‡æ¯”ä¾‹ï¼Œå¦‚ {'è‹å®': 0.4, 'æ‹¼å¤šå¤š': 0.6}
    :return: æ··åˆåçš„å•†å“åˆ—è¡¨
    """
    print("ğŸ¯ å¼€å§‹æ™ºèƒ½æ··åˆå•†å“æ•°æ®...")
    
    # æ•°æ®æ ‡å‡†åŒ–
    suning_list = [standardize_product_data(item) for item in suning_list]
    pdd_list = [standardize_product_data(item) for item in pdd_list]
    
    print(f"åŸå§‹æ•°æ®: è‹å® {len(suning_list)} ä¸ª, æ‹¼å¤šå¤š {len(pdd_list)} ä¸ª")
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¯”ä¾‹ï¼Œä½¿ç”¨éšæœºæ··åˆ
    if not target_ratio:
        return simple_mix_products(suning_list, pdd_list)
    
    # è®¡ç®—ç›®æ ‡æ•°é‡
    total_count = len(suning_list) + len(pdd_list)
    target_suning_count = int(total_count * target_ratio.get('è‹å®', 0.5))
    target_pdd_count = total_count - target_suning_count
    
    # éšæœºé€‰æ‹©å•†å“
    import random
    
    # éšæœºé€‰æ‹©è‹å®å•†å“
    selected_suning = random.sample(suning_list, min(target_suning_count, len(suning_list)))
    
    # éšæœºé€‰æ‹©æ‹¼å¤šå¤šå•†å“
    selected_pdd = random.sample(pdd_list, min(target_pdd_count, len(pdd_list)))
    
    # åˆå¹¶å¹¶éšæœºæ‰“ä¹±
    mixed_products = selected_suning + selected_pdd
    random.shuffle(mixed_products)
    
    print(f"æ™ºèƒ½æ··åˆå®Œæˆ: æ€»å…± {len(mixed_products)} ä¸ªå•†å“")
    
    # ç»Ÿè®¡å®é™…åˆ†å¸ƒ
    actual_suning_count = len([p for p in mixed_products if p['shop_platform'] == 'è‹å®'])
    actual_pdd_count = len([p for p in mixed_products if p['shop_platform'] == 'æ‹¼å¤šå¤š'])
    
    print(f"ç›®æ ‡æ¯”ä¾‹: è‹å® {target_ratio.get('è‹å®', 0.5):.1%}, æ‹¼å¤šå¤š {target_ratio.get('æ‹¼å¤šå¤š', 0.5):.1%}")
    print(f"å®é™…åˆ†å¸ƒ: è‹å® {actual_suning_count} ä¸ª, æ‹¼å¤šå¤š {actual_pdd_count} ä¸ª")
    
    return mixed_products

def mix_and_save_csv(suning_list, pdd_list, csv_filename='mixed_goods.csv', mix_mode='simple'):
    """
    æ¸…æ´—ï¼šå•†å“æ•°æ®å¹¶ä¿å­˜åˆ°CSV
    :param suning_list: è‹å®å•†å“åˆ—è¡¨
    :param pdd_list: æ‹¼å¤šå¤šå•†å“åˆ—è¡¨
    :param csv_filename: è¾“å‡ºCSVæ–‡ä»¶å
    :param mix_mode: æ··åˆæ¨¡å¼ ('simple', 'smart', 'balanced')
    :return: æ··åˆåçš„å•†å“åˆ—è¡¨
    """
    print(f"ğŸ”„ å¼€å§‹æ··åˆå•†å“æ•°æ® (æ¨¡å¼: {mix_mode})...")
    
    if mix_mode == 'simple':
        mixed_products = simple_mix_products(suning_list, pdd_list)
    elif mix_mode == 'smart':
        # æ™ºèƒ½æ··åˆï¼šæ‹¼å¤šå¤šç¨å¤š
        mixed_products = smart_mix_products(suning_list, pdd_list, {'è‹å®': 0.4, 'æ‹¼å¤šå¤š': 0.6})
    elif mix_mode == 'balanced':
        # å¹³è¡¡æ··åˆï¼šå„å ä¸€åŠ
        mixed_products = smart_mix_products(suning_list, pdd_list, {'è‹å®': 0.5, 'æ‹¼å¤šå¤š': 0.5})
    else:
        mixed_products = simple_mix_products(suning_list, pdd_list)
    
    # ä¿å­˜åˆ°CSV
    if save_goods_to_csv(mixed_products, filename=csv_filename):
        print(f"âœ… æ··åˆæ•°æ®å·²ä¿å­˜åˆ° {csv_filename}")
    else:
        print("âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥")
    
    return mixed_products
