from lxml import etree
from time import sleep
from app01.utils.utils_func import draw_num, avoid_check
import time
import random
import csv
from datetime import datetime
import re

def crawler(goods_word, max_pages=1, fast_mode=True):
    """
    è‹å®å•†å“çˆ¬è™«
    :param goods_word: æœç´¢å…³é”®è¯
    :param max_pages: æœ€å¤§ç¿»é¡µæ•°ï¼Œé»˜è®¤2é¡µ
    :param fast_mode: å¿«é€Ÿæ¨¡å¼ï¼Œå‡å°‘å»¶è¿Ÿ
    """
    goods_info = []
    bro = avoid_check()
    
    print(f"å¼€å§‹æœç´¢å…³é”®è¯: {goods_word}")
    bro.get('https://suning.com/')
    
    # æ ‡ç­¾å®šä½
    search_input = bro.find_element('id', value='searchKeywords')
    # æœç´¢å…³é”®è¯
    search_input.send_keys(goods_word)
    # ç‚¹å‡»æœç´¢æŒ‰é’®
    btn = bro.find_element('id', value='searchSubmit')
    btn.submit()
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´å»¶è¿Ÿ
    delay = 1 if fast_mode else random.uniform(2, 4)
    sleep(delay)
    
    # æ£€æŸ¥é¡µé¢ç±»å‹
    page_type = "product"  # é»˜è®¤å•†å“é¡µ
    if "brand.suning.com" in bro.current_url:
        page_type = "brand"
    
    print(f"é¡µé¢ç±»å‹: {page_type}")
    
    # æ‰§è¡Œæ»šåŠ¨åŠ è½½ï¼ˆå‡å°‘æ»šåŠ¨æ¬¡æ•°å’Œå»¶è¿Ÿï¼‰
    scroll_times = 1 if fast_mode else 2
    for i in range(scroll_times):
        bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
        sleep(0.5 if fast_mode else random.uniform(2, 3))
    
    # æ•°æ®è§£æ
    tree = etree.HTML(bro.page_source)
    
    # æ ¹æ®é¡µé¢ç±»å‹è·å–å•†å“åˆ—è¡¨
    if page_type == "product":
        goods_li_list = tree.xpath('//div[@id="product-list"]/ul/li')
    else:  # å“ç‰Œé¡µ
        goods_li_list = tree.xpath('//div[contains(@class, "item-list")]/ul/li')
    
    print(f"ç¬¬ä¸€é¡µæ‰¾åˆ° {len(goods_li_list)} ä¸ªå•†å“")
    
    # ç¿»é¡µå¤„ç†
    for page_num in range(2, max_pages + 1):
        try:
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")
            if page_type == "product":
                btn_next = bro.find_element('id', value='nextPage')
            else:
                btn_next = bro.find_element('xpath', '//a[contains(text(),"ä¸‹ä¸€é¡µ")]')
            
            url = btn_next.get_attribute('href')
            bro.get(url)
            
            # æ ¹æ®æ¨¡å¼è°ƒæ•´å»¶è¿Ÿ
            delay = 0.5 if fast_mode else random.uniform(2, 4)
            sleep(delay)
            
            # æ»šåŠ¨åŠ è½½ï¼ˆå‡å°‘æ»šåŠ¨æ¬¡æ•°ï¼‰
            for j in range(scroll_times):
                bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
                sleep(0.3 if fast_mode else random.uniform(2, 3))
            
            # è§£ææ–°é¡µé¢
            tree = etree.HTML(bro.page_source)
            if page_type == "product":
                new_items = tree.xpath('//div[@id="product-list"]/ul/li')
            else:
                new_items = tree.xpath('//div[contains(@class, "item-list")]/ul/li')
            
            goods_li_list.extend(new_items)
            print(f"ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(new_items)} ä¸ªå•†å“")
            
        except Exception as e:
            print(f"ç¿»é¡µå¤±è´¥: {str(e)}")
            break
    
    # è§£æå•†å“ä¿¡æ¯
    parsed_count = 0
    skipped_count = 0
    
    for i, li in enumerate(goods_li_list):
        try:
            # å•†å“å›¾ç‰‡
            if page_type == "product":
                goods_img = li.xpath('.//div[@class="img-block"]/a/img/@src')[0]
            else:
                goods_img = li.xpath('.//div[contains(@class, "img-block")]/a/img/@src')[0]
            
            goods_img = 'https:' + goods_img if not goods_img.startswith('http') else goods_img
            
            # å•†å“æ ‡é¢˜
            if page_type == "product":
                goods_title = ''.join(li.xpath('.//div[@class="title-selling-point"]/a//text()')).replace('\n', '').strip()
            else:
                goods_title = ''.join(li.xpath('.//div[contains(@class, "title-selling-point")]/a//text()')).replace('\n', '').strip()
            
            # å•†å“ä»·æ ¼
            if page_type == "product":
                price_elem = li.xpath('.//div[@class="price-box"]//text()')
            else:
                price_elem = li.xpath('.//div[contains(@class, "price-box")]//text()')
            
            goods_price = ''.join([p.strip() for p in price_elem if p.strip()])
            goods_price = draw_num(goods_price)
            if not goods_price:
                print(f"âš ï¸ å•†å“ {i+1}: ä»·æ ¼è§£æå¤±è´¥ï¼Œè·³è¿‡")
                skipped_count += 1
                continue
            
            # é”€é‡
            if page_type == "product":
                goods_sales = li.xpath('.//div[@class="info-evaluate"]/a/i/text()')
            else:
                goods_sales = li.xpath('.//div[contains(@class, "info-evaluate")]/a/i/text()')
            
            goods_sales = goods_sales[0] if goods_sales else '0'
            
            # åº—é“ºå
            if page_type == "product":
                shop_elem = li.xpath('.//div[@class="store-stock"]/a/text()')
            else:
                shop_elem = li.xpath('.//div[contains(@class, "store-stock")]/a/text()')
            
            goods_shop = shop_elem[0] if shop_elem else 'æœªçŸ¥'
            
            # å•†å“é“¾æ¥
            if page_type == "product":
                link_str = li.xpath('.//div[@class="title-selling-point"]/a/@sa-data')[0]
                link_list = link_str.split(',')
                link_shop_id = draw_num(link_list[2])
                link_prd_id = draw_num(link_list[1])
                goods_link = f'https://product.suning.com/{link_shop_id}/{link_prd_id}.html'
            else:
                goods_link = li.xpath('.//div[contains(@class, "title-selling-point")]/a/@href')[0]
                goods_link = 'https:' + goods_link if not goods_link.startswith('http') else goods_link
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            goods_info.append({
                'goods_img': goods_img,
                'goods_title': goods_title[:127],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                'goods_price': float(goods_price),
                'goods_sales': goods_sales.replace('+', '') if goods_sales else '0',
                'shop_title': goods_shop,
                'shop_platform': 'è‹å®',
                'goods_link': goods_link,
                'grab_time': time.strftime('%Y-%m-%d %H:%M', time.localtime()),
                'page_type': page_type,  # æ·»åŠ é¡µé¢ç±»å‹æ ‡è¯†
                'search_keyword': goods_word  # æ·»åŠ æœç´¢å…³é”®è¯å­—æ®µ
            })
            parsed_count += 1
            
        except Exception as e:
            print(f"âš ï¸ å•†å“ {i+1}: è§£æå¤±è´¥ - {str(e)}")
            skipped_count += 1
            continue
    
    print(f"ğŸ“Š è§£æç»Ÿè®¡: æˆåŠŸ {parsed_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´æœ€ç»ˆå»¶è¿Ÿ
    if not fast_mode:
        sleep(2)
    
    bro.quit()
    print(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(goods_info)} ä¸ªå•†å“")
    return goods_info
# def save_to_csv(data, filename='suning_products.csv'):
#     """å°†çˆ¬å–çš„æ•°æ®ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
#     if not data:
#         print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
#         return
    
#     # è·å–å­—æ®µåï¼ˆä½¿ç”¨ç¬¬ä¸€æ¡æ•°æ®çš„é”®ï¼‰
#     fieldnames = data[0].keys()
    
#     try:
#         with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
#             writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
#             writer.writeheader()  # å†™å…¥è¡¨å¤´
#             writer.writerows(data)  # å†™å…¥æ‰€æœ‰æ•°æ®
        
#         print(f"æˆåŠŸä¿å­˜ {len(data)} æ¡æ•°æ®åˆ° {filename}")
#     except Exception as e:
#         print(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
def save_to_csv(data, filename='suning_products.csv'):
    """
    å°†çˆ¬å–çš„æ•°æ®ä¿å­˜ä¸ºä¸¥æ ¼ç¬¦åˆRFC 4180æ ‡å‡†çš„CSVæ–‡ä»¶
    ä¸»è¦æ”¹è¿›ï¼š
    1. ä¸¥æ ¼å¤„ç†å­—æ®µä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆé€—å·ã€æ¢è¡Œç¬¦ã€å¼•å·ï¼‰
    2. ç¡®ä¿æ‰€æœ‰å­—æ®µæ­£ç¡®å¼•ç”¨
    3. ç»Ÿä¸€æ¢è¡Œç¬¦ä¸ºCRLF
    4. æ­£ç¡®å¤„ç†ç©ºå€¼å’ŒNone
    5. å¼ºåˆ¶å­—æ®µé¡ºåºä¸€è‡´æ€§
    """
    if not data:
        print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return False
    
    # å®šä¹‰æ ‡å‡†å­—æ®µé¡ºåº
    standard_fields = [
        'goods_img', 'goods_title', 'goods_price', 
        'goods_sales', 'shop_title', 'shop_platform',
        'goods_link', 'grab_time', 'page_type', 'search_keyword'
    ]
    
    try:
        with open(filename, 'w', newline='\r\n', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(
                csvfile, 
                fieldnames=standard_fields,
                delimiter=',',
                quoting=csv.QUOTE_ALL,  # æ‰€æœ‰å­—æ®µéƒ½ç”¨å¼•å·åŒ…è£¹ï¼Œç¡®ä¿å®‰å…¨
                quotechar='"',
                doublequote=True,  # ä½¿ç”¨åŒå¼•å·è½¬ä¹‰å­—æ®µå†…çš„å¼•å·
                escapechar=None,   # ç¦ç”¨åæ–œæ è½¬ä¹‰
                strict=True        # ä¸¥æ ¼æ¨¡å¼ç¡®ä¿å­—æ®µä¸€è‡´æ€§
            )
            
            writer.writeheader()
            
            success_count = 0
            for row in data:
                try:
                    # æ•°æ®æ¸…æ´—å¤„ç†
                    cleaned_row = {
                        'goods_img': clean_url(row.get('goods_img', '')),
                        'goods_title': clean_text(row.get('goods_title', ''), max_length=200),
                        'goods_price': clean_price(row.get('goods_price', 0)),
                        'goods_sales': clean_sales(row.get('goods_sales', '0')),
                        'shop_title': clean_text(row.get('shop_title', 'æœªçŸ¥åº—é“º')),
                        'shop_platform': row.get('shop_platform', 'æœªçŸ¥å¹³å°'),
                        'goods_link': clean_url(row.get('goods_link', '')),
                        'grab_time': row.get('grab_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                        'page_type': row.get('page_type', 'product'),
                        'search_keyword': clean_text(row.get('search_keyword', ''), max_length=50)
                    }
                    
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ˜¯å­—ç¬¦ä¸²ä¸”æ­£ç¡®å¤„ç†Noneå€¼
                    cleaned_row = {k: '' if v is None else str(v) for k, v in cleaned_row.items()}
                    
                    writer.writerow(cleaned_row)
                    success_count += 1
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ•°æ®è¡Œæ—¶å‡ºé”™ï¼ˆè·³è¿‡è¯¥è¡Œï¼‰: {str(e)}")
                    continue
        
        print(f"âœ… æˆåŠŸä¿å­˜ {success_count}/{len(data)} æ¡æ ‡å‡†CSVæ•°æ®åˆ° {filename}")
        return True
    
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        return False

def clean_text(text, max_length=200):
    """ä¸¥æ ¼æ¸…æ´—æ–‡æœ¬å­—æ®µï¼Œç¡®ä¿ç¬¦åˆCSVè§„èŒƒ"""
    if text is None:
        return ''
    
    text = str(text).strip()
    
    # 1. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = ' '.join(text.split())
    
    # 2. é•¿åº¦é™åˆ¶
    if len(text) > max_length:
        text = text[:max_length-3] + '...'
    
    # 3. CSVç‰¹æ®Šå­—ç¬¦å·²åœ¨writerä¸­å¤„ç†ï¼Œè¿™é‡Œä¸å†é‡å¤å¤„ç†
    
    return text

def clean_price(price):
    """ä¸¥æ ¼ä»·æ ¼æ¸…æ´—ï¼Œè¿”å›æ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²"""
    if price is None:
        return '0.00'
    
    try:
        # ç»Ÿä¸€è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†æ ¼å¼åŒ–
        price_float = float(str(price).replace(',', '').strip())
        return '{:.2f}'.format(price_float)
    except (ValueError, TypeError):
        return '0.00'

def clean_url(url):
    """URLæ ‡å‡†åŒ–å¤„ç†ï¼Œç¡®ä¿æœ‰æ•ˆURL"""
    if url is None:
        return ''
    
    url = str(url).strip()
    if not url:
        return ''
    
    if not url.startswith(('http://', 'https://')):
        return f'https:{url}' if url.startswith('//') else f'https://{url}'
    return url

def clean_sales(sales):
    """é”€é‡æ•°æ®æ¸…æ´—ï¼Œè¿”å›æ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²"""
    if sales is None:
        return '0'
    
    sales = str(sales).strip()
    sales = re.sub(r'[^0-9]', '', sales)  # åªä¿ç•™æ•°å­—
    return sales if sales else '0'

if __name__ == "__main__":
    word = "æå®"
    
    # å¯ä»¥é€‰æ‹©å¿«é€Ÿæ¨¡å¼æˆ–æ­£å¸¸æ¨¡å¼
    fast_mode = True  # è®¾ç½®ä¸ºTrueå¯ç”¨å¿«é€Ÿæ¨¡å¼
    
    print("="*50)
    print("è‹å®å•†å“çˆ¬è™«")
    print("="*50)
    print(f"æœç´¢å…³é”®è¯: {word}")
    print(f"å¿«é€Ÿæ¨¡å¼: {'å¼€å¯' if fast_mode else 'å…³é—­'}")
    print("="*50)
    
    sn_goods_info = crawler(goods_word=word, fast_mode=fast_mode)
    save_to_csv(sn_goods_info)
    
    print(f"\nå…±è·å– {len(sn_goods_info)} æ¡å•†å“æ•°æ®")
    for idx, item in enumerate(sn_goods_info[:3], 1):  # æ‰“å°å‰3æ¡ä½œä¸ºç¤ºä¾‹
        print(f"\nå•†å“ {idx}:")
        print(f"ç±»å‹: {'å•†å“é¡µ' if item['page_type'] == 'product' else 'å“ç‰Œé¡µ'}")
        print(f"å…³é”®è¯: {item['search_keyword']}")
        print(f"æ ‡é¢˜: {item['goods_title']}")
        print(f"ä»·æ ¼: {item['goods_price']}")
        print(f"é”€é‡: {item['goods_sales']}")
        print(f"åº—é“º: {item['shop_title']}")
        print(f"é“¾æ¥: {item['goods_link']}")
