from lxml import etree
from time import sleep
from app01.utils.utils_func import draw_num, avoid_check
import time
import random
import csv
from datetime import datetime
import re
import pymysql

# å•†å“åˆ†ç±»å…³é”®è¯æ˜ å°„ - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œè§£å†³å…³é”®è¯å†²çª
CATEGORY_KEYWORDS = {
    'æ‰‹æœºæ•°ç ': ['æ‰‹æœº', 'iPhone', 'åŽä¸º', 'å°ç±³', 'OPPO', 'vivo', 'ä¸‰æ˜Ÿ', 'é­…æ—', 'ä¸€åŠ ', 'realme', 'iQOO', 'è£è€€', 'çº¢ç±³', 'è‹¹æžœ', 'æ™ºèƒ½æ‰‹æœº', 'å¹³æ¿', 'iPad', 'ç”µè„‘', 'ç¬”è®°æœ¬', 'MacBook', 'ThinkPad', 'æˆ´å°”', 'è”æƒ³', 'åŽç¡•', 'æƒ æ™®', 'æ•°ç ', 'ç›¸æœº', 'å•å', 'å¾®å•', 'æ‘„åƒæœº', 'è€³æœº', 'éŸ³å“', 'è“ç‰™è€³æœº', 'æ— çº¿è€³æœº', 'å……ç”µå™¨', 'æ•°æ®çº¿', 'å……ç”µå®', 'ç§»åŠ¨ç”µæº', 'æ™ºèƒ½æ‰‹è¡¨', 'æ‰‹çŽ¯', 'æ•°ç äº§å“'],
    'æœè£…éž‹å¸½': ['è¡£æœ', 'ä¸Šè¡£', 'è£¤å­', 'è£™å­', 'å¤–å¥—', 'ç¾½ç»’æœ', 'æ£‰æœ', 'å«è¡£', 'Tæ¤', 'è¡¬è¡«', 'ç‰›ä»”è£¤', 'è¿åŠ¨è£¤', 'ä¼‘é—²è£¤', 'è¥¿è£…', 'ç¤¼æœ', 'å†…è¡£', 'å†…è£¤', 'è¢œå­', 'éž‹å­', 'è¿åŠ¨éž‹', 'è·‘éž‹', 'ç¯®çƒéž‹', 'è¶³çƒéž‹', 'å¸†å¸ƒéž‹', 'çš®éž‹', 'å‡‰éž‹', 'æ‹–éž‹', 'é´å­', 'å¸½å­', 'æ£’çƒå¸½', 'é¸­èˆŒå¸½', 'æ¯›çº¿å¸½', 'å›´å·¾', 'æ‰‹å¥—', 'è…°å¸¦', 'åŒ…åŒ…', 'èƒŒåŒ…', 'æ‰‹æåŒ…', 'é’±åŒ…', 'åŒè‚©åŒ…', 'æœè£…', 'ç”·è£…', 'å¥³è£…', 'ç«¥è£…', 'æ—¶å°š'],
    'è¿åŠ¨æˆ·å¤–': ['è¿åŠ¨', 'å¥èº«', 'è·‘æ­¥', 'ç¯®çƒ', 'è¶³çƒ', 'ç¾½æ¯›çƒ', 'ä¹’ä¹“çƒ', 'ç½‘çƒ', 'æ¸¸æ³³', 'ç‘œä¼½', 'æˆ·å¤–', 'ç™»å±±', 'å¾’æ­¥', 'éœ²è¥', 'é’“é±¼', 'éª‘è¡Œ', 'æ»‘é›ª', 'æ»‘æ¿', 'è½®æ»‘', 'å¥èº«å™¨æ', 'å“‘é“ƒ', 'è·‘æ­¥æœº', 'åŠ¨æ„Ÿå•è½¦', 'ç‘œä¼½åž«', 'è¿åŠ¨æœ', 'è¿åŠ¨è£¤', 'è¿åŠ¨è¢œ', 'æŠ¤å…·', 'æŠ¤è†', 'æŠ¤è…•', 'æŠ¤è‚˜', 'å¤´ç›”', 'æ‰‹å¥—', 'è¿åŠ¨è£…å¤‡', 'æˆ·å¤–è£…å¤‡', 'ä½“è‚²ç”¨å“'],
    'å®¶å±…ç”Ÿæ´»': ['å®¶å…·', 'æ²™å‘', 'åºŠ', 'æ¡Œå­', 'æ¤…å­', 'æŸœå­', 'è¡£æŸœ', 'ä¹¦æŸœ', 'éž‹æŸœ', 'èŒ¶å‡ ', 'ç”µè§†æŸœ', 'é¤æ¡Œ', 'ä¹¦æ¡Œ', 'åŠžå…¬æ¡Œ', 'åºŠåž«', 'æž•å¤´', 'è¢«å­', 'åºŠå•', 'è¢«å¥—', 'æž•å¥—', 'æ¯›å·¾', 'æµ´å·¾', 'æµ´è¢', 'å®¶å±…', 'å®¶è£…', 'è£…é¥°', 'æ‘†ä»¶', 'èŠ±ç“¶', 'ç›¸æ¡†', 'åœ°æ¯¯', 'çª—å¸˜', 'ç¯å…·', 'å°ç¯', 'åŠç¯', 'å£ç¯', 'å¸é¡¶ç¯', 'å®¶å±…ç”¨å“'],
    'é£Ÿå“é¥®æ–™': ['é›¶é£Ÿ', 'é¥¼å¹²', 'è–¯ç‰‡', 'ç³–æžœ', 'å·§å…‹åŠ›', 'åšæžœ', 'ç“œå­', 'èŠ±ç”Ÿ', 'æ ¸æ¡ƒ', 'æä»', 'è…°æžœ', 'å¼€å¿ƒæžœ', 'é¥®æ–™', 'å¯ä¹', 'é›ªç¢§', 'æžœæ±', 'å¥¶èŒ¶', 'å’–å•¡', 'èŒ¶', 'çŸ¿æ³‰æ°´', 'çº¯å‡€æ°´', 'ç‰›å¥¶', 'é…¸å¥¶', 'é¢åŒ…', 'è›‹ç³•', 'æœˆé¥¼', 'ç²½å­', 'æ–¹ä¾¿é¢', 'ç«è…¿è‚ ', 'ç½å¤´', 'è°ƒå‘³å“', 'é…±æ²¹', 'é†‹', 'ç›', 'ç³–', 'æ²¹', 'ç±³', 'é¢', 'é¢æ¡', 'é£Ÿå“', 'é›¶é£Ÿ', 'å°åƒ', 'é¥®å“'],
    'æ¯å©´ç”¨å“': ['å°¿ä¸æ¹¿', 'çº¸å°¿è£¤', 'æ¹¿å·¾', 'å¥¶ç“¶', 'å¥¶å˜´', 'å¸å¥¶å™¨', 'å©´å„¿è½¦', 'å©´å„¿åºŠ', 'æ‘‡ç¯®', 'çŽ©å…·', 'ç§¯æœ¨', 'æ‹¼å›¾', 'æ¯›ç»’çŽ©å…·', 'ç›Šæ™ºçŽ©å…·', 'æ—©æ•™', 'ç»˜æœ¬', 'æ•…äº‹ä¹¦', 'å©´å„¿æœ', 'è¿žä½“è¡£', 'çˆ¬æœ', 'å›´å˜´', 'å£æ°´å·¾', 'æ¶¦è‚¤éœ²', 'æŠ¤è‡€è†', 'æ¯å©´', 'å©´å„¿', 'å®å®', 'å„¿ç«¥', 'å¹¼å„¿', 'æ¯å©´ç”¨å“'],
    'ç¾Žå¦†æŠ¤è‚¤': ['æŠ¤è‚¤å“', 'æ´—é¢å¥¶', 'çˆ½è‚¤æ°´', 'ç²¾åŽæ¶²', 'é¢éœœ', 'ä¹³æ¶²', 'çœ¼éœœ', 'é¢è†œ', 'é˜²æ™’éœœ', 'éš”ç¦»éœœ', 'ç²‰åº•æ¶²', 'BBéœœ', 'CCéœœ', 'é®ç‘•è†', 'æ•£ç²‰', 'å®šå¦†ç²‰', 'è…®çº¢', 'çœ¼å½±', 'çœ¼çº¿ç¬”', 'ç«æ¯›è†', 'çœ‰ç¬”', 'å£çº¢', 'å”‡è†', 'å”‡å½©', 'æŒ‡ç”²æ²¹', 'é¦™æ°´', 'æŠ¤å‘ç´ ', 'å‘è†œ', 'ç²¾æ²¹', 'ç¾Žå®¹ä»ª', 'åŒ–å¦†åˆ·', 'ç¾Žå¦†è›‹', 'ç¾Žå¦†', 'æŠ¤è‚¤', 'åŒ–å¦†å“', 'å½©å¦†'],
    'å›¾ä¹¦éŸ³åƒ': ['å›¾ä¹¦', 'å°è¯´', 'æ–‡å­¦', 'åŽ†å²', 'å“²å­¦', 'å¿ƒç†å­¦', 'ç»æµŽå­¦', 'ç®¡ç†å­¦', 'è®¡ç®—æœº', 'ç¼–ç¨‹', 'æŠ€æœ¯', 'æ•™æ', 'æ•™è¾…', 'è€ƒè¯•', 'è‹±è¯­', 'æ•°å­¦', 'è¯­æ–‡', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'åœ°ç†', 'æ”¿æ²»', 'éŸ³ä¹', 'CD', 'DVD', 'è“å…‰', 'ç”µå½±', 'ç”µè§†å‰§', 'çºªå½•ç‰‡', 'åŠ¨ç”»', 'æ¸¸æˆ', 'æ‰‹æŸ„', 'é”®ç›˜', 'é¼ æ ‡', 'æ˜¾ç¤ºå™¨', 'éŸ³ç®±', 'ä¹¦ç±', 'æ‚å¿—', 'éŸ³åƒåˆ¶å“'],
    'æ±½è½¦ç”¨å“': ['æ±½è½¦', 'è½®èƒŽ', 'æœºæ²¹', 'æœºæ»¤', 'ç©ºæ»¤', 'æ±½æ»¤', 'åˆ¹è½¦ç‰‡', 'åˆ¹è½¦ç›˜', 'ç«èŠ±å¡ž', 'ç”µç“¶', 'è“„ç”µæ± ', 'é›¨åˆ·', 'é›¨åˆ®å™¨', 'è½¦ç¯', 'å¤§ç¯', 'å°¾ç¯', 'è½¬å‘ç¯', 'é›¾ç¯', 'è½¦è†œ', 'è´´è†œ', 'è„šåž«', 'åº§å¥—', 'æ–¹å‘ç›˜å¥—', 'æŒ‚ä»¶', 'æ‘†ä»¶', 'å¯¼èˆª', 'è¡Œè½¦è®°å½•ä»ª', 'å€’è½¦é›·è¾¾', 'å€’è½¦å½±åƒ', 'è½¦è½½å……ç”µå™¨', 'è½¦è½½å†°ç®±', 'è½¦è½½å¸å°˜å™¨', 'æ±½è½¦é…ä»¶', 'æ±½é…'],
    'åŒ»è¯ä¿å¥': ['è¯å“', 'æ„Ÿå†’è¯', 'é€€çƒ§è¯', 'æ¶ˆç‚Žè¯', 'æ­¢ç—›è¯', 'ç»´ç”Ÿç´ ', 'é’™ç‰‡', 'é±¼æ²¹', 'è›‹ç™½ç²‰', 'ä¿å¥å“', 'è¥å…»å“', 'å‡è‚¥è¯', 'å‡è‚¥èŒ¶', 'å‡è‚¥äº§å“', 'åŒ»ç–—å™¨æ¢°', 'è¡€åŽ‹è®¡', 'è¡€ç³–ä»ª', 'ä½“æ¸©è®¡', 'å¬è¯Šå™¨', 'æŒ‰æ‘©å™¨', 'æŒ‰æ‘©æ¤…', 'æŒ‰æ‘©åž«', 'ç†ç–—ä»ª', 'è‰¾ç¸', 'æ‹”ç½', 'åˆ®ç—§', 'é’ˆç¸', 'ä¸­è¯', 'ä¸­è¯æ', 'ä¸­æˆè¯', 'è¥¿è¯', 'å¤„æ–¹è¯', 'éžå¤„æ–¹è¯', 'åŒ»è¯', 'ä¿å¥', 'åŒ»ç–—']
}

def get_category_from_title(title):
    """
    æ ¹æ®å•†å“æ ‡é¢˜æ™ºèƒ½è¯†åˆ«å•†å“åˆ†ç±»
    """
    if not title:
        return 'æœªåˆ†ç±»'
    
    title_lower = title.lower()
    
    # ç»Ÿè®¡æ¯ä¸ªåˆ†ç±»çš„åŒ¹é…å…³é”®è¯æ•°é‡
    category_scores = {}
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        score = 0
        for keyword in keywords:
            if keyword.lower() in title_lower:
                score += 1
        if score > 0:
            category_scores[category] = score
    
    # è¿”å›žå¾—åˆ†æœ€é«˜çš„åˆ†ç±»
    if category_scores:
        best_category = max(category_scores.items(), key=lambda x: x[1])[0]
        return best_category
    
    return 'æœªåˆ†ç±»'

def get_category_from_page(tree, page_type):
    """
    ä»Žé¡µé¢å…ƒç´ ä¸­æå–åˆ†ç±»ä¿¡æ¯
    """
    try:
        if page_type == "product":
            # å°è¯•ä»Žé¢åŒ…å±‘å¯¼èˆªèŽ·å–åˆ†ç±»
            breadcrumb = tree.xpath('//div[@class="breadcrumb"]//text()')
            if breadcrumb:
                breadcrumb_text = ''.join(breadcrumb).strip()
                # ä»Žé¢åŒ…å±‘ä¸­æå–åˆ†ç±»
                for category in CATEGORY_KEYWORDS.keys():
                    if category in breadcrumb_text:
                        return category
            
            # å°è¯•ä»Žé¡µé¢æ ‡é¢˜èŽ·å–åˆ†ç±»
            page_title = tree.xpath('//title/text()')
            if page_title:
                title_text = page_title[0].strip()
                return get_category_from_title(title_text)
        
        return 'æœªåˆ†ç±»'
    except Exception as e:
        print(f"æå–é¡µé¢åˆ†ç±»æ—¶å‡ºé”™: {str(e)}")
        return 'æœªåˆ†ç±»'

def crawler(goods_word):
    goods_info = []
    bro = avoid_check()
    bro.get('https://suning.com/')
    
    # æ ‡ç­¾å®šä½
    search_input = bro.find_element('id', value='searchKeywords')
    # æœç´¢å…³é”®è¯
    search_input.send_keys(goods_word)
    # ç‚¹å‡»æœç´¢æŒ‰é’®
    btn = bro.find_element('id', value='searchSubmit')
    btn.submit()
    sleep(random.uniform(2, 4))
    
    # æ£€æŸ¥é¡µé¢ç±»åž‹
    page_type = "product"  # é»˜è®¤å•†å“é¡µ
    if "brand.suning.com" in bro.current_url:
        page_type = "brand"
    
    # æ‰§è¡Œæ»šåŠ¨åŠ è½½
    for i in range(1, 3):
        bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
        sleep(random.uniform(2, 3))
    
    # æ•°æ®è§£æž
    tree = etree.HTML(bro.page_source)
    
    # æ ¹æ®é¡µé¢ç±»åž‹èŽ·å–å•†å“åˆ—è¡¨
    if page_type == "product":
        goods_li_list = tree.xpath('//div[@id="product-list"]/ul/li')
    else:  # å“ç‰Œé¡µ
        goods_li_list = tree.xpath('//div[contains(@class, "item-list")]/ul/li')
    
    # ç¿»é¡µå¤„ç†
    for i in range(1, 3):
        try:
            if page_type == "product":
                btn_next = bro.find_element('id', value='nextPage')
            else:
                btn_next = bro.find_element('xpath', '//a[contains(text(),"ä¸‹ä¸€é¡µ")]')
            
            url = btn_next.get_attribute('href')
            if url:
                bro.get(url)
            else:
                print("ä¸‹ä¸€é¡µé“¾æŽ¥ä¸ºç©ºï¼Œåœæ­¢ç¿»é¡µ")
                break
            sleep(random.uniform(2, 4))
            
            # æ»šåŠ¨åŠ è½½
            for j in range(1, 3):
                bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
                sleep(random.uniform(2, 3))
            
            # è§£æžæ–°é¡µé¢
            tree = etree.HTML(bro.page_source)
            if page_type == "product":
                new_items = tree.xpath('//div[@id="product-list"]/ul/li')
            else:
                new_items = tree.xpath('//div[contains(@class, "item-list")]/ul/li')
            
            goods_li_list.extend(new_items)
        except Exception as e:
            print(f"ç¿»é¡µå¤±è´¥: {str(e)}")
            break
    
    # è§£æžå•†å“ä¿¡æ¯
    for li in goods_li_list:
        try:
            # å•†å“å›¾ç‰‡
            if page_type == "product":
                goods_img = li.xpath('.//div[@class="img-block"]/a/img/@src')[0]
            else:
                goods_img = li.xpath('.//div[contains(@class, "img-block")]/a/img/@src')[0]
            
            goods_img = 'https:' + goods_img if not goods_img.startswith('http') else goods_img
            
            # å•†å“æ ‡é¢˜
            if page_type == "product":
                goods_title = ''.join(li.xpath('.//div[@class="title-selling-point"]/a//text()')).replace('\n', '').strip()
            else:
                goods_title = ''.join(li.xpath('.//div[contains(@class, "title-selling-point")]/a//text()')).replace('\n', '').strip()
            
            # å•†å“ä»·æ ¼
            if page_type == "product":
                price_elem = li.xpath('.//div[@class="price-box"]//text()')
            else:
                price_elem = li.xpath('.//div[contains(@class, "price-box")]//text()')
            
            goods_price = ''.join([p.strip() for p in price_elem if p.strip()])
            goods_price = draw_num(goods_price)
            if not goods_price:
                continue
            
            # é”€é‡
            if page_type == "product":
                goods_sales = li.xpath('.//div[@class="info-evaluate"]/a/i/text()')
            else:
                goods_sales = li.xpath('.//div[contains(@class, "info-evaluate")]/a/i/text()')
            
            goods_sales = goods_sales[0] if goods_sales else '0'
            
            # åº—é“ºå
            if page_type == "product":
                shop_elem = li.xpath('.//div[@class="store-stock"]/a/text()')
            else:
                shop_elem = li.xpath('.//div[contains(@class, "store-stock")]/a/text()')
            
            goods_shop = shop_elem[0] if shop_elem else 'æœªçŸ¥'
            
            # å•†å“é“¾æŽ¥
            if page_type == "product":
                link_str = li.xpath('.//div[@class="title-selling-point"]/a/@sa-data')[0]
                link_list = link_str.split(',')
                link_shop_id = draw_num(link_list[2])
                link_prd_id = draw_num(link_list[1])
                goods_link = f'https://product.suning.com/{link_shop_id}/{link_prd_id}.html'
            else:
                goods_link = li.xpath('.//div[contains(@class, "title-selling-point")]/a/@href')[0]
                goods_link = 'https:' + goods_link if not goods_link.startswith('http') else goods_link
            
            # æ™ºèƒ½è¯†åˆ«å•†å“åˆ†ç±»
            category = get_category_from_title(goods_title)
            
            # æ·»åŠ åˆ°ç»“æžœåˆ—è¡¨
            goods_info.append({
                'goods_img': goods_img,
                'goods_title': goods_title[:127],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                'goods_price': float(goods_price),
                'goods_sales': goods_sales.replace('+', '') if goods_sales else '0',
                'shop_title': goods_shop,
                'shop_platform': 'è‹å®',
                'goods_link': goods_link,
                'grab_time': time.strftime('%Y-%m-%d %H:%M', time.localtime()),
                'page_type': page_type,  # æ·»åŠ é¡µé¢ç±»åž‹æ ‡è¯†
                'category': category
            })
        except Exception as e:
            print(f"è§£æžå•†å“æ—¶å‡ºé”™: {str(e)}")
            continue
    
    sleep(2)
    bro.quit()
    return goods_info

def save_to_csv(data, filename='suning_products.csv'):
    """
    å°†çˆ¬å–çš„æ•°æ®ä¿å­˜ä¸ºä¸¥æ ¼ç¬¦åˆRFC 4180æ ‡å‡†çš„CSVæ–‡ä»¶
    ä¸»è¦æ”¹è¿›ï¼š
    1. ä¸¥æ ¼å¤„ç†å­—æ®µä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆé€—å·ã€æ¢è¡Œç¬¦ã€å¼•å·ï¼‰
    2. ç¡®ä¿æ‰€æœ‰å­—æ®µæ­£ç¡®å¼•ç”¨
    3. ç»Ÿä¸€æ¢è¡Œç¬¦ä¸ºCRLF
    4. æ­£ç¡®å¤„ç†ç©ºå€¼å’ŒNone
    5. å¼ºåˆ¶å­—æ®µé¡ºåºä¸€è‡´æ€§
    """
    if not data:
        print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return False
    
    # å®šä¹‰æ ‡å‡†å­—æ®µé¡ºåº
    standard_fields = [
        'goods_img', 'goods_title', 'goods_price', 
        'goods_sales', 'shop_title', 'shop_platform',
        'goods_link', 'grab_time', 'page_type', 'category'
    ]
    
    try:
        with open(filename, 'w', newline='\r\n', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(
                csvfile, 
                fieldnames=standard_fields,
                delimiter=',',
                quoting=csv.QUOTE_ALL,  # æ‰€æœ‰å­—æ®µéƒ½ç”¨å¼•å·åŒ…è£¹ï¼Œç¡®ä¿å®‰å…¨
                quotechar='"',
                doublequote=True,  # ä½¿ç”¨åŒå¼•å·è½¬ä¹‰å­—æ®µå†…çš„å¼•å·
                escapechar=None,   # ç¦ç”¨åæ–œæ è½¬ä¹‰
                strict=True        # ä¸¥æ ¼æ¨¡å¼ç¡®ä¿å­—æ®µä¸€è‡´æ€§
            )
            
            writer.writeheader()
            
            success_count = 0
            for row in data:
                try:
                    # æ•°æ®æ¸…æ´—å¤„ç†
                    cleaned_row = {
                        'goods_img': clean_url(row.get('goods_img', '')),
                        'goods_title': clean_text(row.get('goods_title', ''), max_length=200),
                        'goods_price': clean_price(row.get('goods_price', 0)),
                        'goods_sales': clean_sales(row.get('goods_sales', '0')),
                        'shop_title': clean_text(row.get('shop_title', 'æœªçŸ¥åº—é“º')),
                        'shop_platform': row.get('shop_platform', 'æœªçŸ¥å¹³å°'),
                        'goods_link': clean_url(row.get('goods_link', '')),
                        'grab_time': row.get('grab_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                        'page_type': row.get('page_type', 'product'),
                        'category': row.get('category', 'æœªåˆ†ç±»')
                    }
                    
                    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ˜¯å­—ç¬¦ä¸²ä¸”æ­£ç¡®å¤„ç†Noneå€¼
                    cleaned_row = {k: '' if v is None else str(v) for k, v in cleaned_row.items()}
                    
                    writer.writerow(cleaned_row)
                    success_count += 1
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ•°æ®è¡Œæ—¶å‡ºé”™ï¼ˆè·³è¿‡è¯¥è¡Œï¼‰: {str(e)}")
                    continue
        
        print(f"âœ… æˆåŠŸä¿å­˜ {success_count}/{len(data)} æ¡æ ‡å‡†CSVæ•°æ®åˆ° {filename}")
        return True
    
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        return False

def clean_text(text, max_length=200):
    """ä¸¥æ ¼æ¸…æ´—æ–‡æœ¬å­—æ®µï¼Œç¡®ä¿ç¬¦åˆCSVè§„èŒƒ"""
    if text is None:
        return ''
    
    text = str(text).strip()
    
    # 1. åŽ»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
    text = ' '.join(text.split())
    
    # 2. é•¿åº¦é™åˆ¶
    if len(text) > max_length:
        text = text[:max_length-3] + '...'
    
    # 3. CSVç‰¹æ®Šå­—ç¬¦å·²åœ¨writerä¸­å¤„ç†ï¼Œè¿™é‡Œä¸å†é‡å¤å¤„ç†
    
    return text

def clean_price(price):
    """ä¸¥æ ¼ä»·æ ¼æ¸…æ´—ï¼Œè¿”å›žæ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²"""
    if price is None:
        return '0.00'
    
    try:
        # ç»Ÿä¸€è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†æ ¼å¼åŒ–
        price_float = float(str(price).replace(',', '').strip())
        return '{:.2f}'.format(price_float)
    except (ValueError, TypeError):
        return '0.00'

def clean_url(url):
    """URLæ ‡å‡†åŒ–å¤„ç†ï¼Œç¡®ä¿æœ‰æ•ˆURL"""
    if url is None:
        return ''
    
    url = str(url).strip()
    if not url:
        return ''
    
    if not url.startswith(('http://', 'https://')):
        return f'https:{url}' if url.startswith('//') else f'https://{url}'
    return url

def clean_sales(sales):
    """é”€é‡æ•°æ®æ¸…æ´—ï¼Œè¿”å›žæ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²"""
    if sales is None:
        return '0'
    
    sales = str(sales).strip()
    sales = re.sub(r'[^0-9]', '', sales)  # åªä¿ç•™æ•°å­—
    return sales if sales else '0'

def analyze_category_distribution(goods_list):
    """åˆ†æžå•†å“åˆ†ç±»åˆ†å¸ƒ"""
    category_count = {}
    for item in goods_list:
        category = item.get('category', 'æœªåˆ†ç±»')
        category_count[category] = category_count.get(category, 0) + 1
    
    print("\nðŸ“Š å•†å“åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in sorted(category_count.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(goods_list)) * 100
        print(f"  {category}: {count} ä»¶ ({percentage:.1f}%)")
    
    return category_count

def write_to_mysql(goods):
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='123456',
        database='pricecompare',
        charset='utf8mb4'
    )
    cursor = conn.cursor()

    desc = goods.get('goods_title', 'æš‚æ— æè¿°')
    # ç¡®ä¿å›¾ç‰‡é“¾æŽ¥å®Œæ•´ï¼Œå†™å…¥ img å­—æ®µ
    img = goods.get('goods_img', '')
    if img and not img.startswith('http'):
        img = 'https:' + img if img.startswith('//') else img
    # category
    category = goods.get('category', 'æœªåˆ†ç±»')
    # å“ç‰Œåç”¨åº—é“ºå
    brand_name = goods.get('shop_title', 'æœªçŸ¥å“ç‰Œ')
    cursor.execute("SELECT id FROM brands WHERE name=%s", (brand_name,))
    brand_result = cursor.fetchone()
    if brand_result:
        brand_id = brand_result[0]
    else:
        cursor.execute("INSERT INTO brands (name) VALUES (%s)", (brand_name,))
        brand_id = cursor.lastrowid

    # è‡ªåŠ¨åˆ¤å®š is_hot å’Œ is_drop
    try:
        sales = int(goods.get('goods_sales', '0'))
    except Exception:
        sales = 0
    is_hot = 1 if sales > 2000 else 0

    price_yuan = float(goods['goods_price']) / 100 if float(goods['goods_price']) > 1000 else float(goods['goods_price'])
    is_drop = 1 if price_yuan < 100 else 0

    # æŸ¥æ‰¾æˆ–æ’å…¥ products
    cursor.execute("SELECT id FROM products WHERE title=%s", (goods['goods_title'],))
    result = cursor.fetchone()
    if result:
        product_id = result[0]
    else:
        cursor.execute(
            """INSERT INTO products \
            (title, `desc`, img, category, brand_id, is_hot, is_drop, created_at, updated_at, status)\
            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), NOW(), 1)\
            """, (goods['goods_title'], desc, img, category, brand_id, is_hot, is_drop)
        )
        product_id = cursor.lastrowid

    # æ’å…¥ product_pricesï¼ˆä»·æ ¼è½¬ä¸ºå…ƒï¼Œä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼Œé¿å…é‡å¤
    price_date = goods['grab_time'].split(' ')[0]
    cursor.execute(
        "SELECT id FROM product_prices WHERE product_id=%s AND platform=%s AND date=%s",
        (product_id, goods['shop_platform'], price_date)
    )
    if not cursor.fetchone():
        cursor.execute(
            "INSERT INTO product_prices (product_id, platform, price, date, url, created_at) VALUES (%s, %s, %s, %s, %s, NOW())",
            (product_id, goods['shop_platform'], round(price_yuan, 2), price_date, goods['goods_link'])
        )

    conn.commit()
    cursor.close()
    conn.close()

if __name__ == "__main__":
    # æ”¯æŒæ‰¹é‡å…³é”®è¯é‡‡é›†
    keywords = ["æŽå®", "è€å…‹", "é˜¿è¿ªè¾¾æ–¯", "å®‰è¸", "ç‰¹æ­¥", "é¸¿æ˜Ÿå°”å…‹", "åŒ¹å…‹", "ä¹”ä¸¹", "å½ªé©¬", "æ–ä¹"]
    total_count = 0
    for word in keywords:
        print(f"\nå¼€å§‹é‡‡é›†å…³é”®è¯ï¼š{word}")
        sn_goods_info = crawler(goods_word=word)
        save_to_csv(sn_goods_info, filename=f'suning_products_{word}.csv')
        print(f"å…±èŽ·å– {len(sn_goods_info)} æ¡å•†å“æ•°æ® for {word}")
        
        # åˆ†æžåˆ†ç±»åˆ†å¸ƒ
        analyze_category_distribution(sn_goods_info)
        
        for idx, item in enumerate(sn_goods_info[:3], 1):  # æ‰“å°å‰3æ¡ä½œä¸ºç¤ºä¾‹
            print(f"\nå•†å“ {idx}:")
            print(f"ç±»åž‹: {'å•†å“é¡µ' if item['page_type'] == 'product' else 'å“ç‰Œé¡µ'}")
            print(f"åˆ†ç±»: {item['category']}")
            print(f"æ ‡é¢˜: {item['goods_title']}")
            print(f"ä»·æ ¼: {item['goods_price']}")
            print(f"é”€é‡: {item['goods_sales']}")
            print(f"åº—é“º: {item['shop_title']}")
            print(f"é“¾æŽ¥: {item['goods_link']}")
        # æ–°å¢žï¼šå†™å…¥æ•°æ®åº“
        for item in sn_goods_info:
            write_to_mysql(item)
        total_count += len(sn_goods_info)
    print(f"å·²å†™å…¥ {total_count} æ¡å•†å“æ•°æ®åˆ°æ•°æ®åº“ products å’Œ product_prices è¡¨ã€‚")
