import requests
import time
import random
import json
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains
import pickle
import os
import csv
from datetime import datetime

# 多个User-Agent轮换
USER_AGENTS = [
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',
    'Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Mobile Safari/537.36',
]

# Cookie文件路径
COOKIE_FILE = 'pdd_cookies.pkl'

def save_cookies(driver, filename=COOKIE_FILE):
    """保存cookies到文件"""
    try:
        cookies = driver.get_cookies()
        with open(filename, 'wb') as f:
            pickle.dump(cookies, f)
        print(f"Cookies已保存到 {filename}")
        return True
    except Exception as e:
        print(f"保存cookies失败: {e}")
        return False

def load_cookies(driver, filename=COOKIE_FILE):
    """从文件加载cookies"""
    try:
        if os.path.exists(filename):
            with open(filename, 'rb') as f:
                cookies = pickle.load(f)
            for cookie in cookies:
                try:
                    driver.add_cookie(cookie)
                except:
                    continue
            print(f"Cookies已从 {filename} 加载")
            return True
        else:
            print(f"Cookie文件 {filename} 不存在")
            return False
    except Exception as e:
        print(f"加载cookies失败: {e}")
        return False

def manual_login_guide():
    """手动登录指导"""
    print("\n" + "="*60)
    print("拼多多手动登录指导")
    print("="*60)
    print("\n步骤1：打开浏览器")
    print("1. 启动Edge浏览器")
    print("2. 访问 https://mobile.yangkeduo.com")
    print("\n步骤2：登录账号")
    print("1. 点击右上角的'登录'按钮")
    print("2. 选择登录方式：")
    print("   - 手机号登录")
    print("   - 微信登录")
    print("   - QQ登录")
    print("3. 完成登录验证")
    print("\n步骤3：验证登录状态")
    print("1. 登录成功后，尝试搜索商品")
    print("2. 确认能正常显示搜索结果")
    print("3. 保持浏览器打开状态")
    print("\n步骤4：运行爬虫")
    print("1. 回到命令行")
    print("2. 选择'使用已登录的浏览器'选项")
    print("3. 程序会自动获取登录状态")
    print("="*60)

def get_random_headers():
    """获取随机的请求头"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://mobile.yangkeduo.com/',
        'Origin': 'https://mobile.yangkeduo.com',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
    }

def search_goods_with_login(keyword, page=1, size=45):
    """使用登录状态搜索商品（Edge驱动）"""
    print(f"开始搜索商品: {keyword}")
    
    try:
        # 配置Edge选项
        edge_options = EdgeOptions()
        edge_options.add_argument('--no-sandbox')
        edge_options.add_argument('--disable-dev-shm-usage')
        edge_options.add_argument('--disable-gpu')
        edge_options.add_argument('--window-size=1920,1080')
        edge_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        edge_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        edge_options.add_experimental_option('useAutomationExtension', False)
        
        driver_path = "msedgedriver.exe"  # Edge驱动路径
        driver = webdriver.Edge(service=EdgeService(driver_path), options=edge_options)
        
        # 执行反检测脚本
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # 先访问主页
            print("访问拼多多主页...")
            driver.get('https://mobile.yangkeduo.com')
            time.sleep(3)
            
            # 尝试加载已保存的cookies
            if load_cookies(driver):
                print("使用已保存的登录状态...")
                # 刷新页面以应用cookies
                driver.refresh()
                time.sleep(3)
            else:
                print("没有找到已保存的登录状态")
            
            # 直接搜索商品，不检查登录状态
            search_url = f'https://mobile.yangkeduo.com/search_result.html?search_key={keyword}'
            print(f"正在搜索: {keyword}")
            driver.get(search_url)
            time.sleep(5)
            
            # 保存页面源码用于调试
            with open('pdd_search_result.html', 'w', encoding='utf-8') as f:
                f.write(driver.page_source)
            print("已保存搜索结果页面到 pdd_search_result.html")
            
            # 滑动页面加载更多商品
            print("开始滑动页面加载更多商品...")
            scroll_and_extract_goods(driver, size)
            
            # 提取商品信息
            goods_list = extract_goods_from_search_page(driver, size)
            
            # 如果没找到商品，可能是未登录，提示用户
            if not goods_list:
                print("⚠️ 未找到商品，可能需要登录")
                print("请在浏览器中完成登录，然后按回车继续...")
                input("登录完成后按回车继续...")
                save_cookies(driver)
                
                # 重新搜索
                print("重新搜索商品...")
                driver.get(search_url)
                time.sleep(5)
                goods_list = extract_goods_from_search_page(driver, size)
            
            return goods_list
            
        finally:
            driver.quit()
            
    except Exception as e:
        print(f"搜索商品时出错: {e}")
        return []

def scroll_and_extract_goods(driver, target_count):
    """滑动页面并加载更多商品"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        max_scrolls = 10  # 最大滑动次数
        
        while scroll_count < max_scrolls:
            # 滑动到页面底部
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # 等待页面加载
            
            # 检查是否有新内容加载
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("已到达页面底部，停止滑动")
                break
            
            last_height = new_height
            scroll_count += 1
            
            # 检查当前商品数量
            current_goods = driver.find_elements(By.CSS_SELECTOR, '[class*="product"], [class*="goods"], [class*="item"]')
            print(f"滑动 {scroll_count} 次，当前找到 {len(current_goods)} 个商品")
            
            if len(current_goods) >= target_count:
                print(f"已达到目标商品数量 {target_count}，停止滑动")
                break
            
            # 随机延迟，模拟人工操作
            time.sleep(random.uniform(1, 3))
        
        print(f"滑动完成，共滑动 {scroll_count} 次")
        
    except Exception as e:
        print(f"滑动页面时出错: {e}")

def check_login_status(driver):
    """简化的登录状态检查 - 仅用于调试"""
    try:
        current_url = driver.current_url
        page_title = driver.title
        print(f"当前页面: {current_url}")
        print(f"页面标题: {page_title}")
        
        # 简单检查：如果URL包含login，说明未登录
        if 'login' in current_url.lower():
            print("检测到登录页面")
            return False
        
        print("页面正常，继续执行...")
        return True
            
    except Exception as e:
        print(f"检查页面状态时出错: {e}")
        return True

def extract_goods_from_search_page(driver, size):
    """从搜索结果页面提取商品信息"""
    goods_list = []
    
    # 等待商品列表加载
    time.sleep(5)  # 增加等待时间
    
    print("开始查找商品元素...")
    
    # 只保留有效的选择器
    selectors_to_try = [
        '._3glhOBhU',  # PDD商品元素的主要选择器
    ]
    
    found_elements = False
    for selector in selectors_to_try:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"✅ 使用选择器 '{selector}' 找到 {len(elements)} 个元素")
                found_elements = True
                
                # 提取商品信息
                valid_elements = 0
                for i, element in enumerate(elements[:size]):
                    try:
                        goods = extract_goods_from_element(element)
                        if goods and goods['goods_title'] != "未知商品":
                            goods_list.append(goods)
                            print(f"  📦 商品 {i+1}: {goods['goods_title']} - {goods['goods_price']}")
                            valid_elements += 1
                    except Exception as e:
                        print(f"  ❌ 提取商品 {i+1} 时出错: {e}")
                        continue
                
                print(f"  📊 从 {len(elements)} 个元素中提取到 {valid_elements} 个有效商品")
                
                if goods_list:
                    break
            else:
                print(f"❌ 选择器 '{selector}' 未找到元素")
        except Exception as e:
            print(f"❌ 选择器 '{selector}' 出错: {e}")
            continue
    
    if not found_elements:
        print("⚠️ 所有选择器都未找到任何元素")
        print("正在保存页面源码用于调试...")
        with open('pdd_debug_page.html', 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        print("页面源码已保存到 pdd_debug_page.html")
    
    # 如果没有找到商品，尝试从页面文本中提取
    if not goods_list:
        print("尝试从页面文本中提取商品信息...")
        page_source = driver.page_source
        goods_list = extract_goods_from_page_source(page_source, size)
    
    print(f"🎯 最终提取到 {len(goods_list)} 个商品")
    return goods_list

def extract_goods_from_element(element):
    """从Selenium元素中提取商品信息"""
    try:
        # 获取整个商品卡片的文本内容
        element_text = element.text.strip()
        print(f"[调试] 商品卡片内容：{element_text}")
        
        # 查找商品名称 - 只保留有效的选择器
        name_selectors = [
            '._3ANzdjkc',  # PDD商品标题的主要选择器
            'h3', 'h4', 'h5',
            '[class*="title"]', '[class*="name"]',
        ]
        
        name = "未知商品"
        for selector in name_selectors:
            try:
                name_elem = element.find_element(By.CSS_SELECTOR, selector)
                name = name_elem.text.strip()
                if name and len(name) > 2:
                    print(f"[调试] 使用选择器 {selector} 找到商品名称：{name}")
                    break
            except:
                continue
        
        # 如果选择器都失败了，尝试从整个元素文本中提取商品名称
        if name == "未知商品":
            # 分析元素文本，提取商品名称
            lines = element_text.split('\n')
            for line in lines:
                line = line.strip()
                # 跳过价格、销量等行
                if (line and len(line) > 3 and len(line) < 100 and 
                    not any(keyword in line for keyword in ['¥', '￥', '元', '块', '售', '件', '好评', '分期', '发票'])):
                    name = line
                    print(f"[调试] 从文本中提取商品名称：{name}")
                    break
        
        # 查找价格 - 只保留有效的选择器
        price_selectors = [
            '._3_U04GgA',  # PDD价格的主要选择器
            '[class*="price"]',
        ]
        
        price = "价格未知"
        for selector in price_selectors:
            try:
                price_elem = element.find_element(By.CSS_SELECTOR, selector)
                price_text = price_elem.text.strip()
                if '¥' in price_text or '￥' in price_text or re.search(r'\d+\.?\d*', price_text):
                    price = price_text
                    print(f"[调试] 使用选择器 {selector} 找到价格：{price}")
                    break
            except:
                continue
        
        # 如果选择器都失败了，尝试从文本中提取价格
        if price == "价格未知":
            # 使用正则表达式从文本中提取价格
            price_match = re.search(r'¥\s*(\d+\.?\d*)', element_text)
            if price_match:
                price = f"¥{price_match.group(1)}"
                print(f"[调试] 从文本中提取价格：{price}")
        
        # 优化销量提取逻辑 - 只保留有效的选择器
        sales = "销量未知"
        sales_selectors = [
            '._32q8gNKM',
            '[class*="sales"]',
            '[class*="sold"]',
        ]
        for selector in sales_selectors:
            try:
                elements = element.find_elements(By.CSS_SELECTOR, selector)
                for elem in elements:
                    text = elem.text.strip()
                    if any(k in text for k in ['售', '件', '销量']):
                        sales = text
                        print(f"[调试] 使用选择器 {selector} 找到销量：{sales}")
                        break
                if sales != "销量未知":
                    break
            except:
                continue
        
        # 正则兜底 - 从整个元素文本中提取销量
        if sales == "销量未知":
            match = re.search(r'(总售|已拼|已售|销量)[^\d]*(\d+[万\+]*)(件)?', element_text)
            if match:
                sales = match.group(0)
                print(f"[调试] 从文本中提取销量：{sales}")
        
        # 查找图片
        try:
            img_elem = element.find_element(By.TAG_NAME, 'img')
            img_url = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
        except:
            img_url = ""
        
        # 查找商品ID
        goods_id = None
        try:
            link_elem = element.find_element(By.TAG_NAME, 'a')
            href = link_elem.get_attribute('href')
            match = re.search(r'goods_id=(\d+)', href)
            if match:
                goods_id = match.group(1)
        except:
            pass
        
        # 构建商品信息
        goods_info = {
            'goods_id': goods_id or f"temp_{random.randint(1000, 9999)}",
            'goods_title': clean_title(name),
            'goods_price': clean_price(price),
            'goods_sales': clean_sales(sales),
            'goods_img': clean_url(img_url),
            'shop_title': '拼多多自营',
            'shop_platform': '拼多多',
            'goods_link': clean_url(f'https://mobile.yangkeduo.com/goods.html?goods_id={goods_id}' if goods_id else ''),
            'grab_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'page_type': 'product',
            'search_keyword': '',
            'brand': clean_brand(extract_brand_from_title(name))
        }
        
        print(f"[调试] 提取的商品信息：{goods_info}")
        return goods_info
        
    except Exception as e:
        print(f"提取元素信息时出错: {e}")
        return None

def extract_goods_from_page_source(page_source, size):
    """从页面源码中提取商品信息"""
    goods_list = []
    
    # 使用正则表达式查找商品信息
    # 查找价格模式
    price_pattern = r'¥\s*(\d+\.?\d*)'
    prices = re.findall(price_pattern, page_source)
    
    # 查找可能的商品名称（在价格附近的文本）
    lines = page_source.split('\n')
    
    for i, line in enumerate(lines):
        if re.search(price_pattern, line):
            # 向上查找商品名称
            name = "未知商品"
            for j in range(max(0, i-5), i):
                if j < len(lines):
                    text = lines[j].strip()
                    if text and len(text) > 3 and len(text) < 100:
                        # 排除价格、数字等
                        if not re.search(r'¥|￥|\d+\.?\d*元|\d+\.?\d*块', text):
                            name = text
                            break
            
            price_match = re.search(price_pattern, line)
            if price_match:
                goods = {
                    'goods_id': f"temp_{random.randint(1000, 9999)}",
                    'goods_title': clean_title(name),
                    'goods_price': clean_price(price_match.group(1)),
                    'goods_sales': "销量未知",
                    'goods_img': "",
                    'shop_title': '拼多多自营',
                    'shop_platform': '拼多多',
                    'goods_link': '',
                    'grab_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'page_type': 'product',
                    'search_keyword': '',
                    'brand': clean_brand(extract_brand_from_title(name))
                }
                goods_list.append(goods)
                
                if len(goods_list) >= size:
                    break
    
    return goods_list



def clean_price_pdd(price):
    """提取数字并保留两位小数，单位为元（如299.00）"""
    if not price:
        return ''
    price = str(price)
    price_num = ''.join(c for c in price if c.isdigit() or c == '.')
    try:
        price_float = float(price_num)
        return f"{price_float:.2f}"
    except:
        return ''

def clean_sales_pdd(sales):
    """只保留数字，去掉+、件等"""
    if not sales:
        return '0'
    sales = str(sales)
    sales_num = ''.join(c for c in sales if c.isdigit())
    return sales_num or '0'

def clean_text_pdd(text, max_length=200):
    if not text:
        return ''
    text = str(text).replace('\n', '').replace('\r', '').strip()
    if len(text) > max_length:
        text = text[:max_length-3] + '...'
    return text

def save_to_csv_pdd(data, filename='pdd_products.csv', search_keyword=''):
    """
    将PDD商品数据保存为标准CSV文件，字段顺序：
    goods_img, goods_title, goods_price, goods_sales, shop_title, shop_platform, goods_link, grab_time, page_type, search_keyword
    """
    if not data:
        print("没有数据可保存")
        return False
    
    standard_fields = [
        'goods_img', 'goods_title', 'goods_price', 'goods_sales',
        'shop_title', 'shop_platform', 'goods_link', 'grab_time',
        'page_type', 'search_keyword'
    ]
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=standard_fields, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            success_count = 0
            for row in data:
                mapped = {
                    'goods_img': clean_text_pdd(row.get('goods_img', '')),
                    'goods_title': clean_text_pdd(row.get('goods_title', '')),
                    'goods_price': clean_text_pdd(clean_price_pdd(row.get('goods_price', ''))),
                    'goods_sales': clean_text_pdd(clean_sales_pdd(row.get('goods_sales', ''))),
                    'shop_title': clean_text_pdd(row.get('shop_title', '拼多多自营'), 50),
                    'shop_platform': clean_text_pdd('拼多多'),
                    'goods_link': clean_text_pdd(row.get('goods_link', '')),
                    'grab_time': clean_text_pdd(row.get('grab_time', datetime.now().strftime('%Y-%m-%d %H:%M'))),
                    'page_type': clean_text_pdd('product'),
                    'search_keyword': clean_text_pdd(search_keyword, 50)
                }
                writer.writerow(mapped)
                success_count += 1
        print(f"✅ 成功保存 {success_count}/{len(data)} 条数据到 {filename}")
        return True
    except Exception as e:
        print(f"❌ 保存CSV文件时出错: {e}")
        return False

def extract_brand_from_title(title):
    """从商品标题中提取品牌"""
    if not title:
        return '未知'
    
    # 常见品牌关键词
    brand_keywords = [
        '苹果', 'iPhone', '华为', 'HUAWEI', '小米', 'MI', 'OPPO', 'vivo', '三星', 'SAMSUNG',
        '荣耀', 'HONOR', '联想', 'Lenovo', 'realme', '一加', 'OnePlus', '魅族', 'Meizu',
        '努比亚', 'nubia', '中兴', 'ZTE', '锤子', 'Smartisan', '黑鲨', 'Black Shark',
        '李宁', 'NIKE', '阿迪达斯', 'Adidas', '安踏', 'Anta', '特步', 'Xtep',
        '361度', '361°', '匹克', 'PEAK', '鸿星尔克', 'ERKE', '乔丹', 'QIAODAN'
    ]
    
    title_lower = title.lower()
    for brand in brand_keywords:
        if brand.lower() in title_lower:
            return brand
    
    return '未知'

def clean_title(text, max_length=127):
    """清洗商品标题"""
    if not text:
        return ''
    # 去除HTML标签和特殊字符
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\w\s\u4e00-\u9fff\-\.]', '', text)
    text = ' '.join(text.split())  # 去除多余空格
    return text[:max_length]

def clean_price(price_text):
    """清洗价格"""
    if not price_text:
        return 0.0
    try:
        # 提取数字和小数点
        price_num = re.sub(r'[^\d\.]', '', str(price_text))
        price_float = float(price_num)
        return round(price_float, 2)
    except (ValueError, TypeError):
        return 0.0

def clean_sales(sales_text):
    """清洗销量"""
    if not sales_text:
        return '0'
    # 只保留数字
    sales_num = re.sub(r'[^\d]', '', str(sales_text))
    return sales_num if sales_num else '0'

def clean_brand(brand):
    """清洗品牌"""
    if not brand or brand == '未知':
        return '未知'
    return brand.strip()

def clean_url(url):
    """清洗URL"""
    if not url:
        return ''
    url = str(url).strip()
    if not url.startswith(('http://', 'https://')):
        return f'https:{url}' if url.startswith('//') else f'https://{url}'
    return url

def main():
    """主函数"""
    print("开始运行拼多多爬虫...")
    print("注意：拼多多需要登录才能正常搜索商品")
    print("-" * 50)
    
    # 获取搜索关键词
    keyword = input("请输入搜索关键词（默认：手机）: ").strip() or "手机"
    
    # 获取商品数量
    try:
        size = int(input("请输入要采集的商品数量（默认：10）: ").strip() or "10")
    except:
        size = 30
    
    print(f"开始搜索关键词: {keyword}，目标数量: {size}")
    
    # 使用登录状态搜索商品
    goods_list = search_goods_with_login(keyword, page=1, size=size)
    print(f'共采集到{len(goods_list)}个商品')
    
    if not goods_list:
        print("没有商品数据")
        return
        
    for idx, goods in enumerate(goods_list, 1):
        print(f"\n商品{idx}: {goods['goods_title']}")
        print(f"  价格: {goods['goods_price']}元")
        print(f"  销量: {goods['goods_sales']}")
        print(f"  图片: {goods['goods_img']}")
        print(f"  商品ID: {goods['goods_id']}")
        time.sleep(random.uniform(1, 2))  # 随机延迟
    
    # 保存为CSV
    save_to_csv_pdd(goods_list, filename='pdd_products.csv', search_keyword=keyword)

# 直接执行主函数
if __name__ == '__main__':
    main()
else:
    # 当作为模块导入时也执行主函数
    main()
