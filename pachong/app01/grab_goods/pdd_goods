import requests
import time
import random
import json
from bs4 import BeautifulSoup
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.webdriver.edge.service import Service as EdgeService
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains
import pickle
import os
import csv
from datetime import datetime

# å¤šä¸ªUser-Agentè½®æ¢
USER_AGENTS = [
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',
    'Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Mobile Safari/537.36',
]

# Cookieæ–‡ä»¶è·¯å¾„
COOKIE_FILE = 'pdd_cookies.pkl'

def save_cookies(driver, filename=COOKIE_FILE):
    """ä¿å­˜cookiesåˆ°æ–‡ä»¶"""
    try:
        cookies = driver.get_cookies()
        with open(filename, 'wb') as f:
            pickle.dump(cookies, f)
        print(f"Cookieså·²ä¿å­˜åˆ° {filename}")
        return True
    except Exception as e:
        print(f"ä¿å­˜cookieså¤±è´¥: {e}")
        return False

def load_cookies(driver, filename=COOKIE_FILE):
    """ä»æ–‡ä»¶åŠ è½½cookies"""
    try:
        if os.path.exists(filename):
            with open(filename, 'rb') as f:
                cookies = pickle.load(f)
            for cookie in cookies:
                try:
                    driver.add_cookie(cookie)
                except:
                    continue
            print(f"Cookieså·²ä» {filename} åŠ è½½")
            return True
        else:
            print(f"Cookieæ–‡ä»¶ {filename} ä¸å­˜åœ¨")
            return False
    except Exception as e:
        print(f"åŠ è½½cookieså¤±è´¥: {e}")
        return False

def manual_login_guide():
    """æ‰‹åŠ¨ç™»å½•æŒ‡å¯¼"""
    print("\n" + "="*60)
    print("æ‹¼å¤šå¤šæ‰‹åŠ¨ç™»å½•æŒ‡å¯¼")
    print("="*60)
    print("\næ­¥éª¤1ï¼šæ‰“å¼€æµè§ˆå™¨")
    print("1. å¯åŠ¨Edgeæµè§ˆå™¨")
    print("2. è®¿é—® https://mobile.yangkeduo.com")
    print("\næ­¥éª¤2ï¼šç™»å½•è´¦å·")
    print("1. ç‚¹å‡»å³ä¸Šè§’çš„'ç™»å½•'æŒ‰é’®")
    print("2. é€‰æ‹©ç™»å½•æ–¹å¼ï¼š")
    print("   - æ‰‹æœºå·ç™»å½•")
    print("   - å¾®ä¿¡ç™»å½•")
    print("   - QQç™»å½•")
    print("3. å®Œæˆç™»å½•éªŒè¯")
    print("\næ­¥éª¤3ï¼šéªŒè¯ç™»å½•çŠ¶æ€")
    print("1. ç™»å½•æˆåŠŸåï¼Œå°è¯•æœç´¢å•†å“")
    print("2. ç¡®è®¤èƒ½æ­£å¸¸æ˜¾ç¤ºæœç´¢ç»“æœ")
    print("3. ä¿æŒæµè§ˆå™¨æ‰“å¼€çŠ¶æ€")
    print("\næ­¥éª¤4ï¼šè¿è¡Œçˆ¬è™«")
    print("1. å›åˆ°å‘½ä»¤è¡Œ")
    print("2. é€‰æ‹©'ä½¿ç”¨å·²ç™»å½•çš„æµè§ˆå™¨'é€‰é¡¹")
    print("3. ç¨‹åºä¼šè‡ªåŠ¨è·å–ç™»å½•çŠ¶æ€")
    print("="*60)

def get_random_headers():
    """è·å–éšæœºçš„è¯·æ±‚å¤´"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://mobile.yangkeduo.com/',
        'Origin': 'https://mobile.yangkeduo.com',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-origin',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
    }

def search_goods_with_login(keyword, page=1, size=45):
    """ä½¿ç”¨ç™»å½•çŠ¶æ€æœç´¢å•†å“ï¼ˆEdgeé©±åŠ¨ï¼‰"""
    print(f"å¼€å§‹æœç´¢å•†å“: {keyword}")
    
    try:
        # é…ç½®Edgeé€‰é¡¹
        edge_options = EdgeOptions()
        edge_options.add_argument('--no-sandbox')
        edge_options.add_argument('--disable-dev-shm-usage')
        edge_options.add_argument('--disable-gpu')
        edge_options.add_argument('--window-size=1920,1080')
        edge_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        edge_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        edge_options.add_experimental_option('useAutomationExtension', False)
        
        driver_path = "msedgedriver.exe"  # Edgeé©±åŠ¨è·¯å¾„
        driver = webdriver.Edge(service=EdgeService(driver_path), options=edge_options)
        
        # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # å…ˆè®¿é—®ä¸»é¡µ
            print("è®¿é—®æ‹¼å¤šå¤šä¸»é¡µ...")
            driver.get('https://mobile.yangkeduo.com')
            time.sleep(3)
            
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookies
            if load_cookies(driver):
                print("ä½¿ç”¨å·²ä¿å­˜çš„ç™»å½•çŠ¶æ€...")
                # åˆ·æ–°é¡µé¢ä»¥åº”ç”¨cookies
                driver.refresh()
                time.sleep(3)
            else:
                print("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„ç™»å½•çŠ¶æ€")
            
            # ç›´æ¥æœç´¢å•†å“ï¼Œä¸æ£€æŸ¥ç™»å½•çŠ¶æ€
            search_url = f'https://mobile.yangkeduo.com/search_result.html?search_key={keyword}'
            print(f"æ­£åœ¨æœç´¢: {keyword}")
            driver.get(search_url)
            time.sleep(5)
            
            # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
            with open('pdd_search_result.html', 'w', encoding='utf-8') as f:
                f.write(driver.page_source)
            print("å·²ä¿å­˜æœç´¢ç»“æœé¡µé¢åˆ° pdd_search_result.html")
            
            # æ»‘åŠ¨é¡µé¢åŠ è½½æ›´å¤šå•†å“
            print("å¼€å§‹æ»‘åŠ¨é¡µé¢åŠ è½½æ›´å¤šå•†å“...")
            scroll_and_extract_goods(driver, size)
            
            # æå–å•†å“ä¿¡æ¯
            goods_list = extract_goods_from_search_page(driver, size)
            
            # å¦‚æœæ²¡æ‰¾åˆ°å•†å“ï¼Œå¯èƒ½æ˜¯æœªç™»å½•ï¼Œæç¤ºç”¨æˆ·
            if not goods_list:
                print("âš ï¸ æœªæ‰¾åˆ°å•†å“ï¼Œå¯èƒ½éœ€è¦ç™»å½•")
                print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
                input("ç™»å½•å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                save_cookies(driver)
                
                # é‡æ–°æœç´¢
                print("é‡æ–°æœç´¢å•†å“...")
                driver.get(search_url)
                time.sleep(5)
                goods_list = extract_goods_from_search_page(driver, size)
            
            return goods_list
            
        finally:
            driver.quit()
            
    except Exception as e:
        print(f"æœç´¢å•†å“æ—¶å‡ºé”™: {e}")
        return []

def scroll_and_extract_goods(driver, target_count):
    """æ»‘åŠ¨é¡µé¢å¹¶åŠ è½½æ›´å¤šå•†å“"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        max_scrolls = 10  # æœ€å¤§æ»‘åŠ¨æ¬¡æ•°
        
        while scroll_count < max_scrolls:
            # æ»‘åŠ¨åˆ°é¡µé¢åº•éƒ¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("å·²åˆ°è¾¾é¡µé¢åº•éƒ¨ï¼Œåœæ­¢æ»‘åŠ¨")
                break
            
            last_height = new_height
            scroll_count += 1
            
            # æ£€æŸ¥å½“å‰å•†å“æ•°é‡
            current_goods = driver.find_elements(By.CSS_SELECTOR, '[class*="product"], [class*="goods"], [class*="item"]')
            print(f"æ»‘åŠ¨ {scroll_count} æ¬¡ï¼Œå½“å‰æ‰¾åˆ° {len(current_goods)} ä¸ªå•†å“")
            
            if len(current_goods) >= target_count:
                print(f"å·²è¾¾åˆ°ç›®æ ‡å•†å“æ•°é‡ {target_count}ï¼Œåœæ­¢æ»‘åŠ¨")
                break
            
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººå·¥æ“ä½œ
            time.sleep(random.uniform(1, 3))
        
        print(f"æ»‘åŠ¨å®Œæˆï¼Œå…±æ»‘åŠ¨ {scroll_count} æ¬¡")
        
    except Exception as e:
        print(f"æ»‘åŠ¨é¡µé¢æ—¶å‡ºé”™: {e}")

def check_login_status(driver):
    """ç®€åŒ–çš„ç™»å½•çŠ¶æ€æ£€æŸ¥ - ä»…ç”¨äºè°ƒè¯•"""
    try:
        current_url = driver.current_url
        page_title = driver.title
        print(f"å½“å‰é¡µé¢: {current_url}")
        print(f"é¡µé¢æ ‡é¢˜: {page_title}")
        
        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœURLåŒ…å«loginï¼Œè¯´æ˜æœªç™»å½•
        if 'login' in current_url.lower():
            print("æ£€æµ‹åˆ°ç™»å½•é¡µé¢")
            return False
        
        print("é¡µé¢æ­£å¸¸ï¼Œç»§ç»­æ‰§è¡Œ...")
        return True
            
    except Exception as e:
        print(f"æ£€æŸ¥é¡µé¢çŠ¶æ€æ—¶å‡ºé”™: {e}")
        return True

def extract_goods_from_search_page(driver, size):
    """ä»æœç´¢ç»“æœé¡µé¢æå–å•†å“ä¿¡æ¯"""
    goods_list = []
    
    # ç­‰å¾…å•†å“åˆ—è¡¨åŠ è½½
    time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
    
    print("å¼€å§‹æŸ¥æ‰¾å•†å“å…ƒç´ ...")
    
    # åªä¿ç•™æœ‰æ•ˆçš„é€‰æ‹©å™¨
    selectors_to_try = [
        '._3glhOBhU',  # PDDå•†å“å…ƒç´ çš„ä¸»è¦é€‰æ‹©å™¨
    ]
    
    found_elements = False
    for selector in selectors_to_try:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                found_elements = True
                
                # æå–å•†å“ä¿¡æ¯
                valid_elements = 0
                for i, element in enumerate(elements[:size]):
                    try:
                        goods = extract_goods_from_element(element)
                        if goods and goods['goods_title'] != "æœªçŸ¥å•†å“":
                            goods_list.append(goods)
                            print(f"  ğŸ“¦ å•†å“ {i+1}: {goods['goods_title']} - {goods['goods_price']}")
                            valid_elements += 1
                    except Exception as e:
                        print(f"  âŒ æå–å•†å“ {i+1} æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"  ğŸ“Š ä» {len(elements)} ä¸ªå…ƒç´ ä¸­æå–åˆ° {valid_elements} ä¸ªæœ‰æ•ˆå•†å“")
                
                if goods_list:
                    break
            else:
                print(f"âŒ é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°å…ƒç´ ")
        except Exception as e:
            print(f"âŒ é€‰æ‹©å™¨ '{selector}' å‡ºé”™: {e}")
            continue
    
    if not found_elements:
        print("âš ï¸ æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°ä»»ä½•å…ƒç´ ")
        print("æ­£åœ¨ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•...")
        with open('pdd_debug_page.html', 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        print("é¡µé¢æºç å·²ä¿å­˜åˆ° pdd_debug_page.html")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å•†å“ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
    if not goods_list:
        print("å°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–å•†å“ä¿¡æ¯...")
        page_source = driver.page_source
        goods_list = extract_goods_from_page_source(page_source, size)
    
    print(f"ğŸ¯ æœ€ç»ˆæå–åˆ° {len(goods_list)} ä¸ªå•†å“")
    return goods_list

def extract_goods_from_element(element):
    """ä»Seleniumå…ƒç´ ä¸­æå–å•†å“ä¿¡æ¯"""
    try:
        # è·å–æ•´ä¸ªå•†å“å¡ç‰‡çš„æ–‡æœ¬å†…å®¹
        element_text = element.text.strip()
        print(f"[è°ƒè¯•] å•†å“å¡ç‰‡å†…å®¹ï¼š{element_text}")
        
        # æŸ¥æ‰¾å•†å“åç§° - åªä¿ç•™æœ‰æ•ˆçš„é€‰æ‹©å™¨
        name_selectors = [
            '._3ANzdjkc',  # PDDå•†å“æ ‡é¢˜çš„ä¸»è¦é€‰æ‹©å™¨
            'h3', 'h4', 'h5',
            '[class*="title"]', '[class*="name"]',
        ]
        
        name = "æœªçŸ¥å•†å“"
        for selector in name_selectors:
            try:
                name_elem = element.find_element(By.CSS_SELECTOR, selector)
                name = name_elem.text.strip()
                if name and len(name) > 2:
                    print(f"[è°ƒè¯•] ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å•†å“åç§°ï¼š{name}")
                    break
            except:
                continue
        
        # å¦‚æœé€‰æ‹©å™¨éƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»æ•´ä¸ªå…ƒç´ æ–‡æœ¬ä¸­æå–å•†å“åç§°
        if name == "æœªçŸ¥å•†å“":
            # åˆ†æå…ƒç´ æ–‡æœ¬ï¼Œæå–å•†å“åç§°
            lines = element_text.split('\n')
            for line in lines:
                line = line.strip()
                # è·³è¿‡ä»·æ ¼ã€é”€é‡ç­‰è¡Œ
                if (line and len(line) > 3 and len(line) < 100 and 
                    not any(keyword in line for keyword in ['Â¥', 'ï¿¥', 'å…ƒ', 'å—', 'å”®', 'ä»¶', 'å¥½è¯„', 'åˆ†æœŸ', 'å‘ç¥¨'])):
                    name = line
                    print(f"[è°ƒè¯•] ä»æ–‡æœ¬ä¸­æå–å•†å“åç§°ï¼š{name}")
                    break
        
        # æŸ¥æ‰¾ä»·æ ¼ - åªä¿ç•™æœ‰æ•ˆçš„é€‰æ‹©å™¨
        price_selectors = [
            '._3_U04GgA',  # PDDä»·æ ¼çš„ä¸»è¦é€‰æ‹©å™¨
            '[class*="price"]',
        ]
        
        price = "ä»·æ ¼æœªçŸ¥"
        for selector in price_selectors:
            try:
                price_elem = element.find_element(By.CSS_SELECTOR, selector)
                price_text = price_elem.text.strip()
                if 'Â¥' in price_text or 'ï¿¥' in price_text or re.search(r'\d+\.?\d*', price_text):
                    price = price_text
                    print(f"[è°ƒè¯•] ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä»·æ ¼ï¼š{price}")
                    break
            except:
                continue
        
        # å¦‚æœé€‰æ‹©å™¨éƒ½å¤±è´¥äº†ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼
        if price == "ä»·æ ¼æœªçŸ¥":
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼
            price_match = re.search(r'Â¥\s*(\d+\.?\d*)', element_text)
            if price_match:
                price = f"Â¥{price_match.group(1)}"
                print(f"[è°ƒè¯•] ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼ï¼š{price}")
        
        # ä¼˜åŒ–é”€é‡æå–é€»è¾‘ - åªä¿ç•™æœ‰æ•ˆçš„é€‰æ‹©å™¨
        sales = "é”€é‡æœªçŸ¥"
        sales_selectors = [
            '._32q8gNKM',
            '[class*="sales"]',
            '[class*="sold"]',
        ]
        for selector in sales_selectors:
            try:
                elements = element.find_elements(By.CSS_SELECTOR, selector)
                for elem in elements:
                    text = elem.text.strip()
                    if any(k in text for k in ['å”®', 'ä»¶', 'é”€é‡']):
                        sales = text
                        print(f"[è°ƒè¯•] ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ°é”€é‡ï¼š{sales}")
                        break
                if sales != "é”€é‡æœªçŸ¥":
                    break
            except:
                continue
        
        # æ­£åˆ™å…œåº• - ä»æ•´ä¸ªå…ƒç´ æ–‡æœ¬ä¸­æå–é”€é‡
        if sales == "é”€é‡æœªçŸ¥":
            match = re.search(r'(æ€»å”®|å·²æ‹¼|å·²å”®|é”€é‡)[^\d]*(\d+[ä¸‡\+]*)(ä»¶)?', element_text)
            if match:
                sales = match.group(0)
                print(f"[è°ƒè¯•] ä»æ–‡æœ¬ä¸­æå–é”€é‡ï¼š{sales}")
        
        # æŸ¥æ‰¾å›¾ç‰‡
        try:
            img_elem = element.find_element(By.TAG_NAME, 'img')
            img_url = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
        except:
            img_url = ""
        
        # æŸ¥æ‰¾å•†å“ID
        goods_id = None
        try:
            link_elem = element.find_element(By.TAG_NAME, 'a')
            href = link_elem.get_attribute('href')
            match = re.search(r'goods_id=(\d+)', href)
            if match:
                goods_id = match.group(1)
        except:
            pass
        
        # æ„å»ºå•†å“ä¿¡æ¯
        goods_info = {
            'goods_id': goods_id or f"temp_{random.randint(1000, 9999)}",
            'goods_title': clean_title(name),
            'goods_price': clean_price(price),
            'goods_sales': clean_sales(sales),
            'goods_img': clean_url(img_url),
            'shop_title': 'æ‹¼å¤šå¤šè‡ªè¥',
            'shop_platform': 'æ‹¼å¤šå¤š',
            'goods_link': clean_url(f'https://mobile.yangkeduo.com/goods.html?goods_id={goods_id}' if goods_id else ''),
            'grab_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'page_type': 'product',
            'search_keyword': '',
            'brand': clean_brand(extract_brand_from_title(name))
        }
        
        print(f"[è°ƒè¯•] æå–çš„å•†å“ä¿¡æ¯ï¼š{goods_info}")
        return goods_info
        
    except Exception as e:
        print(f"æå–å…ƒç´ ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None

def extract_goods_from_page_source(page_source, size):
    """ä»é¡µé¢æºç ä¸­æå–å•†å“ä¿¡æ¯"""
    goods_list = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å•†å“ä¿¡æ¯
    # æŸ¥æ‰¾ä»·æ ¼æ¨¡å¼
    price_pattern = r'Â¥\s*(\d+\.?\d*)'
    prices = re.findall(price_pattern, page_source)
    
    # æŸ¥æ‰¾å¯èƒ½çš„å•†å“åç§°ï¼ˆåœ¨ä»·æ ¼é™„è¿‘çš„æ–‡æœ¬ï¼‰
    lines = page_source.split('\n')
    
    for i, line in enumerate(lines):
        if re.search(price_pattern, line):
            # å‘ä¸ŠæŸ¥æ‰¾å•†å“åç§°
            name = "æœªçŸ¥å•†å“"
            for j in range(max(0, i-5), i):
                if j < len(lines):
                    text = lines[j].strip()
                    if text and len(text) > 3 and len(text) < 100:
                        # æ’é™¤ä»·æ ¼ã€æ•°å­—ç­‰
                        if not re.search(r'Â¥|ï¿¥|\d+\.?\d*å…ƒ|\d+\.?\d*å—', text):
                            name = text
                            break
            
            price_match = re.search(price_pattern, line)
            if price_match:
                goods = {
                    'goods_id': f"temp_{random.randint(1000, 9999)}",
                    'goods_title': clean_title(name),
                    'goods_price': clean_price(price_match.group(1)),
                    'goods_sales': "é”€é‡æœªçŸ¥",
                    'goods_img': "",
                    'shop_title': 'æ‹¼å¤šå¤šè‡ªè¥',
                    'shop_platform': 'æ‹¼å¤šå¤š',
                    'goods_link': '',
                    'grab_time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'page_type': 'product',
                    'search_keyword': '',
                    'brand': clean_brand(extract_brand_from_title(name))
                }
                goods_list.append(goods)
                
                if len(goods_list) >= size:
                    break
    
    return goods_list



def clean_price_pdd(price):
    """æå–æ•°å­—å¹¶ä¿ç•™ä¸¤ä½å°æ•°ï¼Œå•ä½ä¸ºå…ƒï¼ˆå¦‚299.00ï¼‰"""
    if not price:
        return ''
    price = str(price)
    price_num = ''.join(c for c in price if c.isdigit() or c == '.')
    try:
        price_float = float(price_num)
        return f"{price_float:.2f}"
    except:
        return ''

def clean_sales_pdd(sales):
    """åªä¿ç•™æ•°å­—ï¼Œå»æ‰+ã€ä»¶ç­‰"""
    if not sales:
        return '0'
    sales = str(sales)
    sales_num = ''.join(c for c in sales if c.isdigit())
    return sales_num or '0'

def clean_text_pdd(text, max_length=200):
    if not text:
        return ''
    text = str(text).replace('\n', '').replace('\r', '').strip()
    if len(text) > max_length:
        text = text[:max_length-3] + '...'
    return text

def save_to_csv_pdd(data, filename='pdd_products.csv', search_keyword=''):
    """
    å°†PDDå•†å“æ•°æ®ä¿å­˜ä¸ºæ ‡å‡†CSVæ–‡ä»¶ï¼Œå­—æ®µé¡ºåºï¼š
    goods_img, goods_title, goods_price, goods_sales, shop_title, shop_platform, goods_link, grab_time, page_type, search_keyword
    """
    if not data:
        print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return False
    
    standard_fields = [
        'goods_img', 'goods_title', 'goods_price', 'goods_sales',
        'shop_title', 'shop_platform', 'goods_link', 'grab_time',
        'page_type', 'search_keyword'
    ]
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=standard_fields, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            success_count = 0
            for row in data:
                mapped = {
                    'goods_img': clean_text_pdd(row.get('goods_img', '')),
                    'goods_title': clean_text_pdd(row.get('goods_title', '')),
                    'goods_price': clean_text_pdd(clean_price_pdd(row.get('goods_price', ''))),
                    'goods_sales': clean_text_pdd(clean_sales_pdd(row.get('goods_sales', ''))),
                    'shop_title': clean_text_pdd(row.get('shop_title', 'æ‹¼å¤šå¤šè‡ªè¥'), 50),
                    'shop_platform': clean_text_pdd('æ‹¼å¤šå¤š'),
                    'goods_link': clean_text_pdd(row.get('goods_link', '')),
                    'grab_time': clean_text_pdd(row.get('grab_time', datetime.now().strftime('%Y-%m-%d %H:%M'))),
                    'page_type': clean_text_pdd('product'),
                    'search_keyword': clean_text_pdd(search_keyword, 50)
                }
                writer.writerow(mapped)
                success_count += 1
        print(f"âœ… æˆåŠŸä¿å­˜ {success_count}/{len(data)} æ¡æ•°æ®åˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def extract_brand_from_title(title):
    """ä»å•†å“æ ‡é¢˜ä¸­æå–å“ç‰Œ"""
    if not title:
        return 'æœªçŸ¥'
    
    # å¸¸è§å“ç‰Œå…³é”®è¯
    brand_keywords = [
        'è‹¹æœ', 'iPhone', 'åä¸º', 'HUAWEI', 'å°ç±³', 'MI', 'OPPO', 'vivo', 'ä¸‰æ˜Ÿ', 'SAMSUNG',
        'è£è€€', 'HONOR', 'è”æƒ³', 'Lenovo', 'realme', 'ä¸€åŠ ', 'OnePlus', 'é­…æ—', 'Meizu',
        'åŠªæ¯”äºš', 'nubia', 'ä¸­å…´', 'ZTE', 'é”¤å­', 'Smartisan', 'é»‘é²¨', 'Black Shark',
        'æå®', 'NIKE', 'é˜¿è¿ªè¾¾æ–¯', 'Adidas', 'å®‰è¸', 'Anta', 'ç‰¹æ­¥', 'Xtep',
        '361åº¦', '361Â°', 'åŒ¹å…‹', 'PEAK', 'é¸¿æ˜Ÿå°”å…‹', 'ERKE', 'ä¹”ä¸¹', 'QIAODAN'
    ]
    
    title_lower = title.lower()
    for brand in brand_keywords:
        if brand.lower() in title_lower:
            return brand
    
    return 'æœªçŸ¥'

def clean_title(text, max_length=127):
    """æ¸…æ´—å•†å“æ ‡é¢˜"""
    if not text:
        return ''
    # å»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\w\s\u4e00-\u9fff\-\.]', '', text)
    text = ' '.join(text.split())  # å»é™¤å¤šä½™ç©ºæ ¼
    return text[:max_length]

def clean_price(price_text):
    """æ¸…æ´—ä»·æ ¼"""
    if not price_text:
        return 0.0
    try:
        # æå–æ•°å­—å’Œå°æ•°ç‚¹
        price_num = re.sub(r'[^\d\.]', '', str(price_text))
        price_float = float(price_num)
        return round(price_float, 2)
    except (ValueError, TypeError):
        return 0.0

def clean_sales(sales_text):
    """æ¸…æ´—é”€é‡"""
    if not sales_text:
        return '0'
    # åªä¿ç•™æ•°å­—
    sales_num = re.sub(r'[^\d]', '', str(sales_text))
    return sales_num if sales_num else '0'

def clean_brand(brand):
    """æ¸…æ´—å“ç‰Œ"""
    if not brand or brand == 'æœªçŸ¥':
        return 'æœªçŸ¥'
    return brand.strip()

def clean_url(url):
    """æ¸…æ´—URL"""
    if not url:
        return ''
    url = str(url).strip()
    if not url.startswith(('http://', 'https://')):
        return f'https:{url}' if url.startswith('//') else f'https://{url}'
    return url

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è¿è¡Œæ‹¼å¤šå¤šçˆ¬è™«...")
    print("æ³¨æ„ï¼šæ‹¼å¤šå¤šéœ€è¦ç™»å½•æ‰èƒ½æ­£å¸¸æœç´¢å•†å“")
    print("-" * 50)
    
    # è·å–æœç´¢å…³é”®è¯
    keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šæ‰‹æœºï¼‰: ").strip() or "æ‰‹æœº"
    
    # è·å–å•†å“æ•°é‡
    try:
        size = int(input("è¯·è¾“å…¥è¦é‡‡é›†çš„å•†å“æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼‰: ").strip() or "10")
    except:
        size = 30
    
    print(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}ï¼Œç›®æ ‡æ•°é‡: {size}")
    
    # ä½¿ç”¨ç™»å½•çŠ¶æ€æœç´¢å•†å“
    goods_list = search_goods_with_login(keyword, page=1, size=size)
    print(f'å…±é‡‡é›†åˆ°{len(goods_list)}ä¸ªå•†å“')
    
    if not goods_list:
        print("æ²¡æœ‰å•†å“æ•°æ®")
        return
        
    for idx, goods in enumerate(goods_list, 1):
        print(f"\nå•†å“{idx}: {goods['goods_title']}")
        print(f"  ä»·æ ¼: {goods['goods_price']}å…ƒ")
        print(f"  é”€é‡: {goods['goods_sales']}")
        print(f"  å›¾ç‰‡: {goods['goods_img']}")
        print(f"  å•†å“ID: {goods['goods_id']}")
        time.sleep(random.uniform(1, 2))  # éšæœºå»¶è¿Ÿ
    
    # ä¿å­˜ä¸ºCSV
    save_to_csv_pdd(goods_list, filename='pdd_products.csv', search_keyword=keyword)

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
if __name__ == '__main__':
    main()
else:
    # å½“ä½œä¸ºæ¨¡å—å¯¼å…¥æ—¶ä¹Ÿæ‰§è¡Œä¸»å‡½æ•°
    main()
